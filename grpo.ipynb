{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10b3ed2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# GRPO from scratch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6286e1",
   "metadata": {},
   "source": [
    "## algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20911f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ccca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def render_prompt(prompt):\n",
    "    template = (\n",
    "        \"You are a helpful math assistant.\\n\"\n",
    "        \"Answer the question and write the final result on a new line as:\\n\"\n",
    "        \"\\\\boxed{ANSWER}\\n\\n\"\n",
    "        f\"Question:\\n{prompt}\\n\\nAnswer:\"\n",
    "    )\n",
    "    return template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7801ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    def __init__(self, n_layers):\n",
    "        self.cache = [None] * n_layers\n",
    "\n",
    "    def get(self, layer_idx):\n",
    "        return self.cache[layer_idx]\n",
    "\n",
    "    def update(self, layer_idx, value):\n",
    "        self.cache[layer_idx] = value\n",
    "\n",
    "    def get_all(self):\n",
    "        return self.cache\n",
    "\n",
    "    def reset(self):\n",
    "        for i in range(len(self.cache)):\n",
    "            self.cache[i] = None\n",
    "\n",
    "def top_p_filter(probas, top_p):\n",
    "    if top_p is None or top_p >= 1.0:\n",
    "        return probas\n",
    "\n",
    "    # Step 4.1: Sort by descending probability\n",
    "    sorted_probas, sorted_idx = torch.sort(probas, dim=1, descending=True)\n",
    "\n",
    "    # Step 4.2: Cumulative sum\n",
    "    cumprobas = torch.cumsum(sorted_probas, dim=1)\n",
    "\n",
    "    # Step 4.3.1: Keep tokens where prefix cumulative mass (before token) is < top_ps\n",
    "    # Example: [0.5, 0.41, 0.09] with top_p=0.9 should keep the first two tokens\n",
    "    prefix = cumprobas - sorted_probas   # cumulative mass before each token\n",
    "    keep = prefix < top_p\n",
    "    # Always keep at least one token (fallback for very small/non-positive top_p)\n",
    "    keep[:, 0] = True\n",
    "\n",
    "    # Step 4.3.2: Zero out beyond cutoff\n",
    "    kept_sorted = torch.where(\n",
    "        keep, sorted_probas,\n",
    "        torch.zeros_like(sorted_probas)\n",
    "    )\n",
    "    # Step 4.3.3: Map back to original order\n",
    "    filtered = torch.zeros_like(probas).scatter(1, sorted_idx, kept_sorted)\n",
    "\n",
    "    # Step 4.4: Renormalize to sum to 1\n",
    "    denom = torch.sum(filtered, dim=1).clamp_min(1e-12)\n",
    "    return filtered / denom\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_response(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    device,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "):\n",
    "    input_ids = torch.tensor(\n",
    "        tokenizer.encode(prompt),\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "    cache = KVCache(n_layers=model.cfg[\"n_layers\"])\n",
    "    model.reset_kv_cache()\n",
    "    logits = model(input_ids.unsqueeze(0), cache=cache)[:, -1]\n",
    "\n",
    "    generated = []\n",
    "    for _ in range(max_new_tokens):\n",
    "        if temperature and temperature != 1.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        probas = top_p_filter(probas, top_p)\n",
    "        next_token = torch.multinomial(\n",
    "            probas.cpu(), num_samples=1\n",
    "        ).to(device)\n",
    "\n",
    "        if (\n",
    "            tokenizer.eos_token_id is not None\n",
    "            and next_token.item() == tokenizer.eos_token_id\n",
    "        ):\n",
    "            break\n",
    "        generated.append(next_token.item())\n",
    "        logits = model(next_token, cache=cache)[:, -1]\n",
    "\n",
    "    full_token_ids = torch.cat(\n",
    "        [input_ids,\n",
    "         torch.tensor(generated, device=device, dtype=input_ids.dtype),]\n",
    "    )\n",
    "    return full_token_ids, input_ids.numel(), tokenizer.decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31725a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sympy import simplify\n",
    "from sympy.parsing import sympy_parser as spp\n",
    "from sympy.core.sympify import SympifyError\n",
    "from sympy.polys.polyerrors import PolynomialError\n",
    "from tokenize import TokenError\n",
    "\n",
    "RE_NUMBER = re.compile(\n",
    "    r\"-?(?:\\d+/\\d+|\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?)\"\n",
    ")\n",
    "\n",
    "LATEX_FIXES = [  # Latex formatting to be replaced\n",
    "    (r\"\\\\left\\s*\", \"\"),\n",
    "    (r\"\\\\right\\s*\", \"\"),\n",
    "    (r\"\\\\,|\\\\!|\\\\;|\\\\:\", \"\"),\n",
    "    (r\"\\\\cdot\", \"*\"),\n",
    "    (r\"\\u00B7|\\u00D7\", \"*\"),\n",
    "    (r\"\\\\\\^\\\\circ\", \"\"),\n",
    "    (r\"\\\\dfrac\", r\"\\\\frac\"),\n",
    "    (r\"\\\\tfrac\", r\"\\\\frac\"),\n",
    "    (r\"°\", \"\"),\n",
    "]\n",
    "\n",
    "RE_SPECIAL = re.compile(r\"<\\|[^>]+?\\|>\")  # strip chat special tokens like <|assistant|>\n",
    "SUPERSCRIPT_MAP = {\n",
    "    \"⁰\": \"0\", \"¹\": \"1\", \"²\": \"2\", \"³\": \"3\", \"⁴\": \"4\",\n",
    "    \"⁵\": \"5\", \"⁶\": \"6\", \"⁷\": \"7\", \"⁸\": \"8\", \"⁹\": \"9\",\n",
    "    \"⁺\": \"+\", \"⁻\": \"-\", \"⁽\": \"(\", \"⁾\": \")\",\n",
    "}\n",
    "\n",
    "def get_last_boxed(text):\n",
    "    # Find the last occurrence of \"\\boxed\"\n",
    "    boxed_start_idx = text.rfind(r\"\\boxed\")\n",
    "    if boxed_start_idx == -1:\n",
    "        return None\n",
    "\n",
    "    # Get position after \"\\boxed\"\n",
    "    current_idx = boxed_start_idx + len(r\"\\boxed\")\n",
    "\n",
    "    # Skip any whitespace after \"\\boxed\"\n",
    "    while current_idx < len(text) and text[current_idx].isspace():\n",
    "        current_idx += 1\n",
    "\n",
    "    # Expect an opening brace \"{\"\n",
    "    if current_idx >= len(text) or text[current_idx] != \"{\":\n",
    "        return None\n",
    "\n",
    "    # Parse the braces with nesting\n",
    "    current_idx += 1\n",
    "    brace_depth = 1\n",
    "    content_start_idx = current_idx\n",
    "\n",
    "    while current_idx < len(text) and brace_depth > 0:\n",
    "        char = text[current_idx]\n",
    "        if char == \"{\":\n",
    "            brace_depth += 1\n",
    "        elif char == \"}\":\n",
    "            brace_depth -= 1\n",
    "        current_idx += 1\n",
    "\n",
    "    # Account for unbalanced braces\n",
    "    if brace_depth != 0:\n",
    "        return None\n",
    "\n",
    "    # Extract content inside the outermost braces\n",
    "    return text[content_start_idx:current_idx-1]\n",
    "\n",
    "def extract_final_candidate(text, fallback=\"number_then_full\"):\n",
    "    # Default return value if nothing matches\n",
    "    result = \"\"\n",
    "\n",
    "    if text:\n",
    "        # Prefer the last boxed expression if present\n",
    "        boxed = get_last_boxed(text.strip())\n",
    "        if boxed:\n",
    "            result = boxed.strip().strip(\"$ \")\n",
    "\n",
    "        # If no boxed expression, try fallback\n",
    "        elif fallback in (\"number_then_full\", \"number_only\"):\n",
    "            m = RE_NUMBER.findall(text)\n",
    "            if m:\n",
    "                # Use last number\n",
    "                result = m[-1]\n",
    "            elif fallback == \"number_then_full\":\n",
    "                # Else return full text if no number found\n",
    "                result = text\n",
    "    return result\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = RE_SPECIAL.sub(\"\", text).strip()\n",
    "\n",
    "    # Strip leading multiple-choice labels\n",
    "    # E.g., like \"c. 3\" -> 3, or \"b: 2\" -> 2\n",
    "    match = re.match(r\"^[A-Za-z]\\s*[.:]\\s*(.+)$\", text)\n",
    "    if match:\n",
    "        text = match.group(1)\n",
    "\n",
    "    # Remove angle-degree markers\n",
    "    text = re.sub(r\"\\^\\s*\\{\\s*\\\\circ\\s*\\}\", \"\", text)   # ^{\\circ}\n",
    "    text = re.sub(r\"\\^\\s*\\\\circ\", \"\", text)             # ^\\circ\n",
    "    text = text.replace(\"°\", \"\")                        # Unicode degree\n",
    "\n",
    "    # unwrap \\text{...} if the whole string is wrapped\n",
    "    match = re.match(r\"^\\\\text\\{(?P<x>.+?)\\}$\", text)\n",
    "    if match:\n",
    "        text = match.group(\"x\")\n",
    "\n",
    "    # strip inline/display math wrappers \\( \\) \\[ \\]\n",
    "    text = re.sub(r\"\\\\\\(|\\\\\\)|\\\\\\[|\\\\\\]\", \"\", text)\n",
    "\n",
    "    # light LaTeX canonicalization\n",
    "    for pat, rep in LATEX_FIXES:\n",
    "        text = re.sub(pat, rep, text)\n",
    "\n",
    "    # convert unicode superscripts into exponent form (e.g., 2² -> 2**2)\n",
    "    def convert_superscripts(s, base=None):\n",
    "        converted = \"\".join(\n",
    "            SUPERSCRIPT_MAP[ch] if ch in SUPERSCRIPT_MAP else ch\n",
    "            for ch in s\n",
    "        )\n",
    "        if base is None:\n",
    "            return converted\n",
    "        return f\"{base}**{converted}\"\n",
    "\n",
    "    text = re.sub(\n",
    "        r\"([0-9A-Za-z\\)\\]\\}])([⁰¹²³⁴⁵⁶⁷⁸⁹⁺⁻]+)\",\n",
    "        lambda m: convert_superscripts(m.group(2), base=m.group(1)),\n",
    "        text,\n",
    "    )\n",
    "    text = convert_superscripts(text)\n",
    "\n",
    "    # numbers/roots\n",
    "    text = text.replace(\"\\\\%\", \"%\").replace(\"$\", \"\").replace(\"%\", \"\")\n",
    "    text = re.sub(\n",
    "        r\"\\\\sqrt\\s*\\{([^}]*)\\}\",\n",
    "        lambda match: f\"sqrt({match.group(1)})\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"\\\\sqrt\\s+([^\\\\\\s{}]+)\",\n",
    "        lambda match: f\"sqrt({match.group(1)})\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    # fractions\n",
    "    text = re.sub(\n",
    "        r\"\\\\frac\\s*\\{([^{}]+)\\}\\s*\\{([^{}]+)\\}\",\n",
    "        lambda match: f\"({match.group(1)})/({match.group(2)})\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"\\\\frac\\s+([^\\s{}]+)\\s+([^\\s{}]+)\",\n",
    "        lambda match: f\"({match.group(1)})/({match.group(2)})\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    # exponent and mixed numbers\n",
    "    text = text.replace(\"^\", \"**\")\n",
    "    text = re.sub(\n",
    "        r\"(?<=\\d)\\s+(\\d+/\\d+)\",\n",
    "        lambda match: \"+\" + match.group(1),\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    # 1,234 -> 1234\n",
    "    text = re.sub(\n",
    "        r\"(?<=\\d),(?=\\d\\d\\d(\\D|$))\",\n",
    "        \"\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    return text.replace(\"{\", \"\").replace(\"}\", \"\").strip().lower()\n",
    "\n",
    "\n",
    "def sympy_parser(expr):\n",
    "    # To avoid crashing on long garbage responses\n",
    "    # that some badly trained models (chapter 6) may emit\n",
    "    if expr is None or len(expr) > 2000:\n",
    "        return None\n",
    "    try:\n",
    "        return spp.parse_expr(\n",
    "            expr,\n",
    "            transformations=(\n",
    "                # Standard transformations like handling parentheses\n",
    "                *spp.standard_transformations,\n",
    "\n",
    "                # Allow omitted multiplication symbols (e.g., \"2x\" -> 2*x\")\n",
    "                spp.implicit_multiplication_application,\n",
    "            ),\n",
    "\n",
    "            # Evaluate during parsing so simple constants simplify (e.g., 2+3 -> 5)\n",
    "            evaluate=True,\n",
    "        )\n",
    "    except (SympifyError, SyntaxError, TypeError, AttributeError,\n",
    "            IndexError, TokenError, ValueError, PolynomialError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def equality_check(expr_gtruth, expr_pred):\n",
    "    # First, check if the two expressions are exactly the same string\n",
    "    if expr_gtruth == expr_pred:\n",
    "        return True\n",
    "\n",
    "    # Parse both expressions into SymPy objects (returns None if parsing fails)\n",
    "    gtruth, pred = sympy_parser(expr_gtruth), sympy_parser(expr_pred)\n",
    "\n",
    "    # If both expressions were parsed successfully, try symbolic comparison\n",
    "    if gtruth is not None and pred is not None:\n",
    "        try:\n",
    "            # If the difference is 0, they are equivalent\n",
    "            return simplify(gtruth - pred) == 0\n",
    "        except (SympifyError, TypeError):\n",
    "            pass\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def split_into_parts(text):\n",
    "    result = [text]\n",
    "\n",
    "    if text:\n",
    "        # Check if text looks like a tuple or list, e.g. \"(a, b)\" or \"[a, b]\"\n",
    "        if (\n",
    "            len(text) >= 2\n",
    "            and text[0] in \"([\" and text[-1] in \")]\"\n",
    "            and \",\" in text[1:-1]\n",
    "        ):\n",
    "            # Split on commas inside brackets and strip whitespace\n",
    "            items = [p.strip() for p in text[1:-1].split(\",\")]\n",
    "            if all(items):\n",
    "                result = items\n",
    "    else:\n",
    "        # If text is empty, return an empty list\n",
    "        result = []\n",
    "\n",
    "    return result\n",
    "\n",
    "def grade_answer(pred_text, gt_text):\n",
    "    result = False  # Default outcome if checks fail\n",
    "\n",
    "    # Only continue if both inputs are non-empty strings\n",
    "    if pred_text is not None and gt_text is not None:\n",
    "        gt_parts = split_into_parts(\n",
    "            normalize_text(gt_text)\n",
    "        )  # Break ground truth into comparable parts\n",
    "\n",
    "        pred_parts = split_into_parts(\n",
    "            normalize_text(pred_text)\n",
    "        )  # Break prediction into comparable parts\n",
    "\n",
    "        # Ensure both sides have same number of valid parts\n",
    "        if (gt_parts and pred_parts\n",
    "           and len(gt_parts) == len(pred_parts)):\n",
    "            result = all(\n",
    "                equality_check(gt, pred)\n",
    "                for gt, pred in zip(gt_parts, pred_parts)\n",
    "            )  # Check each part for mathematical equivalence\n",
    "\n",
    "    return result  # True only if all checks passed\n",
    "\n",
    "\n",
    "def reward_rlvr(answer_text, ground_truth):\n",
    "    extracted = extract_final_candidate(\n",
    "        answer_text, fallback=None  # Require \\boxed{}\n",
    "    )\n",
    "    if not extracted:\n",
    "        return 0.0\n",
    "    correct = grade_answer(extracted, ground_truth)\n",
    "    return float(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5cba415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(model, token_ids, prompt_len):\n",
    "    logits = model(token_ids.unsqueeze(0)).squeeze(0).float()\n",
    "    logprobs = torch.log_softmax(logits, dim=-1)\n",
    "    selected = logprobs[:-1].gather(\n",
    "        1, token_ids[1:].unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "    return torch.sum(selected[prompt_len - 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8789d",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "$$\n",
    "\\mathscr{L}(\\theta) = \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afe9af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grpo_loss(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    example,\n",
    "    device,\n",
    "    num_rollouts=2,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "):\n",
    "    assert num_rollouts >= 2\n",
    "    roll_logps, roll_rewards, samples = [], [], []\n",
    "    prompt = render_prompt(example[\"problem\"])\n",
    "\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    for _ in range(num_rollouts):\n",
    "        # Stage 1: generate rollouts\n",
    "        token_ids, prompt_len, text = sample_response(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            device=device,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        # Stage 2: compute rewards\n",
    "        reward = reward_rlvr(text, example[\"answer\"])\n",
    "        \n",
    "        # Stage 4: compute logprobs\n",
    "        logp = sequence_logprob(model, token_ids, prompt_len)\n",
    "\n",
    "        roll_logps.append(logp)\n",
    "        roll_rewards.append(reward)\n",
    "        samples.append(\n",
    "            {\n",
    "                \"text\": text,\n",
    "                \"reward\": reward,\n",
    "                \"gen_len\": token_ids.numel() - prompt_len,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    # Stage 2: collect all rewards\n",
    "    rewards = torch.tensor(roll_rewards, device=device)\n",
    "\n",
    "    # Stage 3: compute advantages\n",
    "    advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-4)\n",
    "\n",
    "    # Stage 4: collect all logprobs\n",
    "    logps = torch.stack(roll_logps)\n",
    "\n",
    "    # Stage 5: compute policy gradient loss\n",
    "    pg_loss = -(advantages.detach() * logps).mean()\n",
    "    loss = pg_loss  # In the next chapter we add a KL term here\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss.item(),\n",
    "        \"pg_loss\": pg_loss.item(),\n",
    "        \"rewards\": roll_rewards,\n",
    "        \"advantages\": advantages.detach().cpu().tolist(),\n",
    "        \"samples\": samples,\n",
    "        \"loss_tensor\": loss,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268310bf",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f0762f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def train_rlvr_grpo(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    math_data,\n",
    "    device,\n",
    "    steps=None,\n",
    "    num_rollouts=2,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    lr=1e-5,\n",
    "    checkpoint_every=50,\n",
    "    checkpoint_dir=\".\",\n",
    "    csv_log_path=None,\n",
    "\n",
    "):\n",
    "    if steps is None:\n",
    "        steps = len(math_data)\n",
    "\n",
    "    # Stage 1: initialize optimize\n",
    "    # (the model was already initialized outside the function)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    current_step = 0\n",
    "    if csv_log_path is None:\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        csv_log_path = f\"train_rlvr_grpo_metrics_{timestamp}.csv\"\n",
    "    csv_log_path = Path(csv_log_path)\n",
    "\n",
    "    try:\n",
    "        # Stage 2: Iterate over training steps\n",
    "        for step in range(steps):\n",
    "\n",
    "            # Stage 3: Reset loss gradient\n",
    "            # (it's best practice to do this at the beginning of each step)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_step = step + 1\n",
    "            example = math_data[step % len(math_data)]\n",
    "\n",
    "            # Stage 4: calculate GRPO loss\n",
    "            stats = compute_grpo_loss(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                example=example,\n",
    "                device=device,\n",
    "                num_rollouts=num_rollouts,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "            )\n",
    "\n",
    "            # Stage 5: Backward pass to calculate loss gradients\n",
    "            stats[\"loss_tensor\"].backward()\n",
    "\n",
    "            # Clip large gradients to improve training stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Stage 6: Update model weights using loss gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Stage 7: Collect rewards, response lengths, and losses\n",
    "            reward_avg = torch.tensor(stats[\"rewards\"]).mean().item()\n",
    "            step_tokens = sum(\n",
    "                sample[\"gen_len\"] for sample in stats[\"samples\"]\n",
    "            )\n",
    "            avg_response_len = (\n",
    "                step_tokens / len(stats[\"samples\"]) \n",
    "                if stats[\"samples\"] else 0.0\n",
    "            )\n",
    "            append_csv_metrics(\n",
    "                csv_log_path, current_step, steps, stats[\"loss\"],\n",
    "                reward_avg, avg_response_len,\n",
    "            )\n",
    "\n",
    "            # Print step metrics\n",
    "            print(\n",
    "                f\"[Step {current_step}/{steps}] \"\n",
    "                f\"loss={stats['loss']:.4f} \"\n",
    "                f\"reward_avg={reward_avg:.3f} \"\n",
    "                f\"avg_resp_len={avg_response_len:.1f}\"\n",
    "            )\n",
    "\n",
    "            # Sample outputs (every 10 steps) to check if model\n",
    "            # generates coherent text\n",
    "            if current_step % 10 == 0:\n",
    "                print(f\"[Step {current_step}] sample outputs\")\n",
    "                for i, sample in enumerate(stats[\"samples\"][:3]):\n",
    "                    text = sample[\"text\"].replace(\"\\n\", \"\\\\n\")\n",
    "                    print(\n",
    "                        f\"  {i+1}) reward={sample['reward']:.3f} \"\n",
    "                        f\"len={sample['gen_len']}: {text}\"\n",
    "                    )\n",
    "                print()\n",
    "\n",
    "            # Stage 8: Save model checkpoint\n",
    "            if checkpoint_every and current_step % checkpoint_every == 0:\n",
    "                ckpt_path = save_checkpoint(\n",
    "                    model=model,\n",
    "                    checkpoint_dir=checkpoint_dir,\n",
    "                    step=current_step,\n",
    "                )\n",
    "                print(f\"Saved checkpoint to {ckpt_path}\")\n",
    "\n",
    "    # Save a model checkpoint if we interrupt the training early\n",
    "    except KeyboardInterrupt:\n",
    "        # ckpt_path = save_checkpoint(\n",
    "        #     model=model,\n",
    "        #     checkpoint_dir=checkpoint_dir,\n",
    "        #     step=max(1, current_step),\n",
    "        #     suffix=\"interrupt\",\n",
    "        # )\n",
    "        # print(f\"\\nKeyboardInterrupt. Saved checkpoint to {ckpt_path}\")\n",
    "        return model\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_checkpoint(model, checkpoint_dir, step, suffix=\"\"):\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    suffix = f\"-{suffix}\" if suffix else \"\"\n",
    "    ckpt_path = (\n",
    "        checkpoint_dir /\n",
    "        f\"qwen3-0.6B-rlvr-grpo-step{step:05d}{suffix}.pth\"\n",
    "    )\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "    return ckpt_path\n",
    "\n",
    "\n",
    "def append_csv_metrics(\n",
    "    csv_log_path,\n",
    "    step_idx,\n",
    "    total_steps,\n",
    "    loss,\n",
    "    reward_avg,\n",
    "    avg_response_len,\n",
    "):\n",
    "    if not csv_log_path.exists():\n",
    "        csv_log_path.write_text(\n",
    "            \"step,total_steps,loss,reward_avg,avg_response_len\\n\",\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "    with csv_log_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\n",
    "            f\"{step_idx},{total_steps},{loss:.6f},{reward_avg:.6f},\"\n",
    "            f\"{avg_response_len:.6f}\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96e64a",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a92a1",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14d8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3Tokenizer:\n",
    "    _SPECIALS = [\n",
    "        \"<|endoftext|>\",\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|object_ref_start|>\", \"<|object_ref_end|>\",\n",
    "        \"<|box_start|>\", \"<|box_end|>\",\n",
    "        \"<|quad_start|>\", \"<|quad_end|>\",\n",
    "        \"<|vision_start|>\", \"<|vision_end|>\",\n",
    "        \"<|vision_pad|>\", \"<|image_pad|>\", \"<|video_pad|>\",\n",
    "    ]\n",
    "    _SPLIT_RE = re.compile(r\"(<\\|[^>]+?\\|>)\")\n",
    "\n",
    "    def __init__(self, tokenizer_file_path=\"tokenizer-base.json\",\n",
    "                 apply_chat_template=False,\n",
    "                 add_generation_prompt=False,\n",
    "                 add_thinking=False):\n",
    "        from tokenizers import Tokenizer\n",
    "\n",
    "        self.apply_chat_template = apply_chat_template\n",
    "        self.add_generation_prompt = add_generation_prompt\n",
    "        self.add_thinking = add_thinking\n",
    "\n",
    "        tok_path = Path(tokenizer_file_path)\n",
    "        if not tok_path.is_file():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Tokenizer file '{tok_path}' not found. Please ensure it's available.\"\n",
    "            )\n",
    "\n",
    "        self._tok = Tokenizer.from_file(str(tok_path))\n",
    "        self._special_to_id = {t: self._tok.token_to_id(t) for t in self._SPECIALS}\n",
    "\n",
    "        self.pad_token = \"<|endoftext|>\"\n",
    "        self.pad_token_id = self._special_to_id.get(self.pad_token)\n",
    "\n",
    "        # Match HF behavior: chat model → <|im_end|>, base model → <|endoftext|>\n",
    "        fname = tok_path.name.lower()\n",
    "        if \"base\" in fname and \"reasoning\" not in fname:\n",
    "            self.eos_token = \"<|endoftext|>\"\n",
    "        else:\n",
    "            self.eos_token = \"<|im_end|>\"\n",
    "        self.eos_token_id = self._special_to_id.get(self.eos_token)\n",
    "\n",
    "    def encode(self, prompt, chat_wrapped=None):\n",
    "        if chat_wrapped is None:\n",
    "            chat_wrapped = self.apply_chat_template\n",
    "\n",
    "        stripped = prompt.strip()\n",
    "        if stripped in self._special_to_id and \"\\n\" not in stripped:\n",
    "            return [self._special_to_id[stripped]]\n",
    "\n",
    "        if chat_wrapped:\n",
    "            prompt = self._wrap_chat(prompt)\n",
    "\n",
    "        ids = []\n",
    "        for part in filter(None, self._SPLIT_RE.split(prompt)):\n",
    "            if part in self._special_to_id:\n",
    "                ids.append(self._special_to_id[part])\n",
    "            else:\n",
    "                ids.extend(self._tok.encode(part).ids)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return self._tok.decode(token_ids, skip_special_tokens=False)\n",
    "\n",
    "    def _wrap_chat(self, user_msg):\n",
    "        s = f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
    "        if self.add_generation_prompt:\n",
    "            s += \"<|im_start|>assistant\"\n",
    "            if self.add_thinking:\n",
    "                s += \"\\n\"  # insert no <think> tag, just a new line\n",
    "            else:\n",
    "                s += \"\\n<think>\\n\\n</think>\\n\\n\"\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127874f7",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b93c9b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length, dtype=dtype)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0)  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "def apply_rope(x, cos, sin, offset=0):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2:]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[offset:offset + seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[offset:offset + seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    # It's ok to use lower-precision after applying cos and sin rotation\n",
    "    return x_rotated.to(dtype=x.dtype)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-6, bias=False, qwen3_compatible=True):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.qwen3_compatible = qwen3_compatible\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "\n",
    "        if self.qwen3_compatible:\n",
    "            x = x.to(torch.float32)\n",
    "\n",
    "        variance = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        norm_x = x * torch.rsqrt(variance + self.eps)\n",
    "        norm_x = norm_x * self.scale\n",
    "\n",
    "        if self.shift is not None:\n",
    "            norm_x = norm_x + self.shift\n",
    "\n",
    "        return norm_x.to(input_dtype)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            qk_norm=cfg[\"qk_norm\"],\n",
    "            dtype=cfg[\"dtype\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "\n",
    "    def forward(self, x, mask, cos, sin, start_pos=0, cache=None):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x, next_cache = self.att(x, mask, cos, sin, start_pos=start_pos, cache=cache)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x, next_cache\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = nn.functional.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False, dtype=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        if head_dim is None:\n",
    "            assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
    "            head_dim = d_in // num_heads\n",
    "\n",
    "        self.head_dim = head_dim\n",
    "        self.d_out = num_heads * head_dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
    "\n",
    "        if qk_norm:\n",
    "            self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "            self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "        else:\n",
    "            self.q_norm = self.k_norm = None\n",
    "\n",
    "    def forward(self, x, mask, cos, sin, start_pos=0, cache=None):\n",
    "        b, num_tokens, _ = x.shape\n",
    "\n",
    "        # Apply projections\n",
    "        queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)\n",
    "        keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        # Reshape to heads / kv-groups\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys_new = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "        values_new = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Optional normalization\n",
    "        if self.q_norm:\n",
    "            queries = self.q_norm(queries)\n",
    "        if self.k_norm:\n",
    "            keys_new = self.k_norm(keys_new)\n",
    "\n",
    "        # Apply RoPE\n",
    "        queries = apply_rope(queries, cos, sin, offset=start_pos)\n",
    "        keys_new = apply_rope(keys_new, cos, sin, offset=start_pos)\n",
    "\n",
    "        if cache is not None:\n",
    "            prev_k, prev_v = cache\n",
    "            keys = torch.cat([prev_k, keys_new], dim=2)\n",
    "            values = torch.cat([prev_v, values_new], dim=2)\n",
    "        else:\n",
    "            start_pos = 0  # reset RoPE\n",
    "            keys, values = keys_new, values_new\n",
    "        next_cache = (keys, values)\n",
    "\n",
    "        # Expand K and V to match number of heads\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        # Attention\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
    "\n",
    "        context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
    "        return self.out_proj(context), next_cache\n",
    "\n",
    "class Qwen3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # Main model parameters\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.trf_blocks = nn.ModuleList(  # ModuleList since Sequential can only accept one input, and we need `x, mask, cos, sin`\n",
    "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "        # Reusable utilities\n",
    "        if cfg[\"head_dim\"] is None:\n",
    "            head_dim = cfg[\"emb_dim\"] // cfg[\"n_heads\"]\n",
    "        else:\n",
    "            head_dim = cfg[\"head_dim\"]\n",
    "        cos, sin = compute_rope_params(\n",
    "            head_dim=head_dim,\n",
    "            theta_base=cfg[\"rope_base\"],\n",
    "            context_length=cfg[\"context_length\"]\n",
    "        )\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "        self.cfg = cfg\n",
    "        self.current_pos = 0  # Track current position in KV cache\n",
    "\n",
    "    def forward(self, in_idx, cache=None):\n",
    "        # Forward pass\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds\n",
    "\n",
    "        num_tokens = x.shape[1]\n",
    "        if cache is not None:\n",
    "            pos_start = self.current_pos\n",
    "            pos_end = pos_start + num_tokens\n",
    "            self.current_pos = pos_end\n",
    "            mask = torch.triu(\n",
    "                torch.ones(pos_end, pos_end, device=x.device, dtype=torch.bool), diagonal=1\n",
    "            )[pos_start:pos_end, :pos_end]\n",
    "        else:\n",
    "            pos_start = 0  # Not strictly necessary but helps torch.compile\n",
    "            mask = torch.triu(\n",
    "                torch.ones(num_tokens, num_tokens, device=x.device, dtype=torch.bool), diagonal=1\n",
    "            )\n",
    "        # Prefill (no cache): mask starts as (num_tokens, num_tokens)\n",
    "        # Cached decoding: mask starts as (num_tokens, prev_k_number_tokens + num_tokens)\n",
    "        #\n",
    "        # We add two leading dimensions so the mask becomes\n",
    "        # (1, 1, num_tokens, num_tokens) during prefill and\n",
    "        # (1, 1, num_tokens, total_key_tokens) during cached decoding.\n",
    "        # These extra dimensions let PyTorch broadcast the same mask\n",
    "        # across all batches and attention heads when applying it to\n",
    "        # attn_scores of shape (batch, num_heads, num_tokens, total_key_tokens).\n",
    "        mask = mask[None, None, :, :]  # broadcast mask\n",
    "\n",
    "        for i, block in enumerate(self.trf_blocks):\n",
    "            blk_cache = cache.get(i) if cache else None\n",
    "            x, new_blk_cache = block(x, mask, self.cos, self.sin,\n",
    "                                     start_pos=pos_start,\n",
    "                                     cache=blk_cache)\n",
    "            if cache is not None:\n",
    "                cache.update(i, new_blk_cache)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
    "        return logits\n",
    "\n",
    "    def reset_kv_cache(self):\n",
    "        self.current_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a918c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_device(enable_tensor_cores=True):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using NVIDIA CUDA GPU\")\n",
    "\n",
    "        if enable_tensor_cores:\n",
    "            major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "            if (major, minor) >= (2, 9):\n",
    "                torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "                torch.backends.cudnn.conv.fp32_precision = \"tf32\"\n",
    "            else:\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True\n",
    "                torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple Silicon GPU (MPS)\")\n",
    "\n",
    "    elif torch.xpu.is_available():\n",
    "        device = torch.device(\"xpu\")\n",
    "        print(\"Using Intel GPU\")\n",
    "\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    return device\n",
    "\n",
    "def download_file(url, out_dir=\".\", backup_url=None):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    filename = Path(urlparse(url).path).name\n",
    "    dest = out_dir / filename\n",
    "\n",
    "    def try_download(u):\n",
    "        try:\n",
    "            with requests.get(u, stream=True, timeout=30) as r:\n",
    "                r.raise_for_status()\n",
    "                size_remote = int(r.headers.get(\"Content-Length\", 0))\n",
    "\n",
    "                # Skip download if already complete\n",
    "                if dest.exists() and size_remote and dest.stat().st_size == size_remote:\n",
    "                    print(f\"✓ {dest} already up-to-date\")\n",
    "                    return True\n",
    "\n",
    "                # Download in 1 MiB chunks with progress display\n",
    "                block = 1024 * 1024\n",
    "                downloaded = 0\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=block):\n",
    "                        if not chunk:\n",
    "                            continue\n",
    "                        f.write(chunk)\n",
    "                        downloaded += len(chunk)\n",
    "                        if size_remote:\n",
    "                            pct = downloaded * 100 // size_remote\n",
    "                            sys.stdout.write(\n",
    "                                f\"\\r{filename}: {pct:3d}% \"\n",
    "                                f\"({downloaded // (1024*1024)} MiB / \"\n",
    "                                f\"{size_remote // (1024*1024)} MiB)\"\n",
    "                            )\n",
    "                            sys.stdout.flush()\n",
    "                if size_remote:\n",
    "                    sys.stdout.write(\"\\n\")\n",
    "            return True\n",
    "        except requests.RequestException:\n",
    "            return False\n",
    "\n",
    "    # Try main URL first\n",
    "    if try_download(url):\n",
    "        return dest\n",
    "\n",
    "    # Try backup URL if provided\n",
    "    if backup_url:\n",
    "        print(f\"Primary URL ({url}) failed.\\nTrying backup URL ({backup_url})...\")\n",
    "        if try_download(backup_url):\n",
    "            return dest\n",
    "\n",
    "    raise RuntimeError(f\"Failed to download {filename} from both mirrors.\")\n",
    "\n",
    "\n",
    "def download_qwen3_small(kind=\"base\", tokenizer_only=False, out_dir=\".\"):\n",
    "    files = {\n",
    "        \"base\": {\"model\": \"qwen3-0.6B-base.pth\", \"tokenizer\": \"tokenizer-base.json\"},\n",
    "        \"reasoning\": {\"model\": \"qwen3-0.6B-reasoning.pth\", \"tokenizer\": \"`tokenizer-reasoning.json`\"},\n",
    "    }\n",
    "    if kind not in files:\n",
    "        raise ValueError(\"kind must be 'base' or 'reasoning'\")\n",
    "\n",
    "    repo = \"rasbt/qwen3-from-scratch\"\n",
    "    hf_fmt = \"https://huggingface.co/{repo}/resolve/main/{file}\"\n",
    "    backup_root = \"https://f001.backblazeb2.com/file/reasoning-from-scratch/qwen3-0.6B\"\n",
    "    targets = [\"tokenizer\"] if tokenizer_only else [\"model\", \"tokenizer\"]\n",
    "\n",
    "    for key in targets:\n",
    "        fname = files[kind][key]\n",
    "        primary = hf_fmt.format(repo=repo, file=fname)\n",
    "        backup = f\"{backup_root}/{fname}\"\n",
    "        download_file(primary, out_dir=out_dir, backup_url=backup)\n",
    "\n",
    "\n",
    "\n",
    "QWEN_CONFIG_06_B = {\n",
    "    \"vocab_size\": 151_936,     # Vocabulary size\n",
    "    \"context_length\": 40_960,  # Length originally used during training\n",
    "    \"emb_dim\": 1024,           # Embedding dimension\n",
    "    \"n_heads\": 16,             # Number of attention heads\n",
    "    \"n_layers\": 28,            # Number of layers\n",
    "    \"hidden_dim\": 3072,        # Size of intermediate dim in FeedForward\n",
    "    \"head_dim\": 128,           # Size of the heads in GQA\n",
    "    \"qk_norm\": True,           # Whether to normalize queries & keys in GQA\n",
    "    \"n_kv_groups\": 8,          # Key-Value groups for GQA\n",
    "    \"rope_base\": 1_000_000.0,  # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,   # Lower-precision dtype to reduce memory\n",
    "}\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(which_model, device, use_compile, local_dir=\"qwen3\"):\n",
    "    if which_model == \"base\":\n",
    "\n",
    "        download_qwen3_small(\n",
    "            kind=\"base\", tokenizer_only=False, out_dir=local_dir\n",
    "        )\n",
    "\n",
    "        tokenizer_path = Path(local_dir) / \"tokenizer-base.json\"\n",
    "        model_path = Path(local_dir) / \"qwen3-0.6B-base.pth\"\n",
    "        tokenizer = Qwen3Tokenizer(tokenizer_file_path=tokenizer_path)\n",
    "\n",
    "    elif which_model == \"reasoning\":\n",
    "\n",
    "        download_qwen3_small(\n",
    "            kind=\"reasoning\", tokenizer_only=False, out_dir=local_dir\n",
    "        )\n",
    "\n",
    "        tokenizer_path = Path(local_dir) / \"tokenizer-reasoning.json\"\n",
    "        model_path = Path(local_dir) / \"qwen3-0.6B-reasoning.pth\"\n",
    "        tokenizer = Qwen3Tokenizer(\n",
    "            tokenizer_file_path=tokenizer_path,\n",
    "            apply_chat_template=True,\n",
    "            add_generation_prompt=True,\n",
    "            add_thinking=True,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid choice: which_model={which_model}\")\n",
    "\n",
    "    model = Qwen3Model(QWEN_CONFIG_06_B)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    if use_compile:\n",
    "        torch._dynamo.config.allow_unspec_int_on_nn_module = True\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f1eb67",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54c9dfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 12000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "def load_math_train(local_path=\"math_train.json\", save_copy=True):\n",
    "    local_path = Path(local_path)\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"math_full_minus_math500/refs/heads/main/\"\n",
    "        \"math_full_minus_math500.json\"\n",
    "    )\n",
    "\n",
    "    if local_path.exists():\n",
    "        with local_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "\n",
    "        if save_copy:  # Saves a local copy\n",
    "            with local_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "\n",
    "    return data\n",
    "\n",
    "math_train = load_math_train()\n",
    "\n",
    "print(\"Dataset size:\", len(math_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c697aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA CUDA GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3Model(\n",
       "  (tok_emb): Embedding(151936, 1024)\n",
       "  (trf_blocks): ModuleList(\n",
       "    (0-27): 28 x TransformerBlock(\n",
       "      (att): GroupedQueryAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc2): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc3): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# model, tokenizer = load_model_and_tokenizer(\n",
    "#     which_model=\"base\",\n",
    "#     device=device,\n",
    "#     use_compile=False\n",
    "# )\n",
    "tokenizer_path = \"qwen3/tokenizer-base.json\"\n",
    "model_path = \"qwen3/qwen3-0.6B-base.pth\"\n",
    "tokenizer = Qwen3Tokenizer(tokenizer_file_path=tokenizer_path)\n",
    "model = Qwen3Model(QWEN_CONFIG_06_B)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331a6a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/50] loss=10.2222 reward_avg=0.250 avg_resp_len=236.8\n",
      "[Step 2/50] loss=12.5457 reward_avg=0.250 avg_resp_len=367.0\n",
      "[Step 3/50] loss=3.4448 reward_avg=0.250 avg_resp_len=37.5\n",
      "[Step 4/50] loss=-2.3608 reward_avg=0.500 avg_resp_len=164.2\n",
      "[Step 5/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=352.8\n",
      "Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00005.pth\n",
      "[Step 6/50] loss=-3.6666 reward_avg=0.250 avg_resp_len=494.2\n",
      "[Step 7/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=512.0\n",
      "[Step 8/50] loss=3.1384 reward_avg=0.750 avg_resp_len=173.5\n",
      "[Step 9/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=405.5\n",
      "[Step 10/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=486.2\n",
      "[Step 10] sample outputs\n",
      "  1) reward=0.000 len=477:  To determine the amount of money Tim should invest to reach a total of \\$60,000 at the end of 5 years with an annual interest rate of 7% compounded quarterly, we can use the formula for compound interest:\\n\\n\\[\\nA = P \\left(1 + \\frac{r}{n}\\right)^{nt}\\n\\]\\n\\nWhere:\\n- \\( A \\) is the amount of money accumulated after \\( t \\) years, including interest.\\n- \\( P \\) is the principal amount (the initial amount of money).\\n- \\( r \\) is the annual interest rate (decimal).\\n- \\( n \\) is the number of times that interest is compounded per year.\\n- \\( t \\) is the time the money is invested for in years.\\n\\nGiven:\\n- \\( A = \\$60,000 \\)\\n- \\( r = 7\\% = 0.07 \\)\\n- \\( n = 4 \\) (since interest is compounded quarterly)\\n- \\( t = 5 \\) years\\n\\nWe need to solve for \\( P \\):\\n\\n\\[\\n60,000 = P \\left(1 + \\frac{0.07}{4}\\right)^{4 \\times 5}\\n\\]\\n\\nFirst, calculate the quarterly interest rate:\\n\\n\\[\\n\\frac{0.07}{4} = 0.0175\\n\\]\\n\\nNext, calculate the total number of compounding periods:\\n\\n\\[\\n4 \\times 5 = 20\\n\\]\\n\\nNow, plug these values into the formula:\\n\\n\\[\\n60,000 = P \\left(1 + 0.0175\\right)^{20}\\n\\]\\n\\nCalculate \\( (1 + 0.0175)^{20} \\):\\n\\n\\[\\n(1.0175)^{20} \\approx 1.4206\\n\\]\\n\\nNow, solve for \\( P \\):\\n\\n\\[\\nP = \\frac{60,000}{1.4206} \\approx 42,160\\n\\]\\n\\nRounding to the nearest dollar:\\n\\n\\[\\nP \\approx \\$42,160\\n\\]\\n\\n**Final Answer:**  \\n\\boxed{42160}\n",
      "  2) reward=0.000 len=467:  To determine how much Tim should invest to reach \\$60,000 in 5 years with a 7% annual interest rate compounded quarterly, we can use the **compound interest formula**:\\n\\n\\[\\nA = P \\left(1 + \\frac{r}{n}\\right)^{nt}\\n\\]\\n\\nWhere:\\n- \\( A \\) = the amount of money accumulated after \\( t \\) years, including interest.\\n- \\( P \\) = the principal amount (the initial amount of money).\\n- \\( r \\) = annual interest rate (in decimal).\\n- \\( n \\) = number of times that interest is compounded per year.\\n- \\( t \\) = time the money is invested for in years.\\n\\n**Given:**\\n- \\( A = \\$60,000 \\)\\n- \\( r = 7\\% = 0.07 \\)\\n- \\( n = 4 \\) (since interest is compounded quarterly)\\n- \\( t = 5 \\) years\\n\\n**Step 1: Plug the values into the formula**\\n\\n\\[\\n60,000 = P \\left(1 + \\frac{0.07}{4}\\right)^{4 \\times 5}\\n\\]\\n\\n**Step 2: Simplify the equation**\\n\\n\\[\\n60,000 = P \\left(1 + 0.0175\\right)^{20}\\n\\]\\n\\n\\[\\n60,000 = P \\left(1.0175\\right)^{20}\\n\\]\\n\\n**Step 3: Calculate \\( (1.0175)^{20} \\)**\\n\\nUsing a calculator:\\n\\n\\[\\n(1.0175)^{20} \\approx 1.4142\\n\\]\\n\\n**Step 4: Solve for \\( P \\)**\\n\\n\\[\\nP = \\frac{60,000}{1.4142} \\approx 42,246.01\\n\\]\\n\\n**Step 5: Round to the nearest dollar**\\n\\n\\[\\nP \\approx \\$42,246\\n\\]\\n\\n**Final Answer:**\\n\\n\\[\\n\\boxed{42246}\\n\\]\n",
      "  3) reward=0.000 len=510:  To determine how much Tim should invest initially to reach a total of \\$60,000 at the end of 5 years with an annual interest rate of 7% compounded quarterly, we can use the formula for compound interest:\\n\\n\\[\\nA = P \\left(1 + \\frac{r}{n}\\right)^{nt}\\n\\]\\n\\nWhere:\\n- \\( A \\) is the amount of money accumulated after \\( t \\) years, including interest.\\n- \\( P \\) is the principal amount (the initial amount of money).\\n- \\( r \\) is the annual interest rate (decimal).\\n- \\( n \\) is the number of times interest is compounded per year.\\n- \\( t \\) is the time the money is invested for in years.\\n\\nGiven:\\n- \\( A = \\$60,000 \\)\\n- \\( r = 7\\% = 0.07 \\)\\n- \\( n = 4 \\) (since the interest is compounded quarterly)\\n- \\( t = 5 \\) years\\n\\nWe need to solve for \\( P \\):\\n\\n\\[\\n60,000 = P \\left(1 + \\frac{0.07}{4}\\right)^{4 \\times 5}\\n\\]\\n\\nFirst, calculate the interest rate per period:\\n\\n\\[\\n\\frac{0.07}{4} = 0.0175\\n\\]\\n\\nNext, calculate the total number of compounding periods:\\n\\n\\[\\n4 \\times 5 = 20\\n\\]\\n\\nNow, plug these values into the compound interest formula:\\n\\n\\[\\n60,000 = P \\left(1 + 0.0175\\right)^{20}\\n\\]\\n\\nCalculate \\( (1 + 0.0175)^{20} \\):\\n\\n\\[\\n1.0175^{20} \\approx 1.4147\\n\\]\\n\\nSo the equation becomes:\\n\\n\\[\\n60,000 = P \\times 1.4147\\n\\]\\n\\nSolve for \\( P \\):\\n\\n\\[\\nP = \\frac{60,000}{1.4147} \\approx 42,359.66\\n\\]\\n\\nRounding to the nearest dollar:\\n\\n\\[\\nP \\approx 42,359\\n\\]\\n\\n**Final Answer:**\\n\\n\\[\\n\\boxed{42359}\\n\\]\n",
      "\n",
      "Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00010.pth\n",
      "[Step 11/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=315.0\n",
      "[Step 12/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=422.0\n",
      "[Step 13/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=193.5\n",
      "[Step 14/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=222.0\n",
      "[Step 15/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=239.5\n",
      "Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00015.pth\n",
      "[Step 16/50] loss=-0.1161 reward_avg=0.750 avg_resp_len=273.2\n",
      "[Step 17/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=405.2\n",
      "[Step 18/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=389.8\n",
      "[Step 19/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=403.2\n",
      "[Step 20/50] loss=-3.2502 reward_avg=0.500 avg_resp_len=467.0\n",
      "[Step 20] sample outputs\n",
      "  1) reward=0.000 len=509:  To determine how long it takes six faucets to fill a 25-gallon tub, we first need to find the rate at which the faucets fill the tub. \\n\\n1. **Find the rate of one faucet:**\\n   - Three faucets fill a 100-gallon tub in 6 minutes.\\n   - Total volume filled by three faucets in 6 minutes: 100 gallons.\\n   - Number of faucets: 3.\\n   - Time taken: 6 minutes.\\n\\n   The rate of one faucet is:\\n   \\[\\n   \\text{Rate of one faucet} = \\frac{\\text{Total volume filled}}{\\text{Number of faucets} \\times \\text{Time}} = \\frac{100 \\text{ gallons}}{3 \\times 6 \\text{ minutes}} = \\frac{100}{18} \\text{ gallons per minute}\\n   \\]\\n\\n2. **Find the rate of six faucets:**\\n   - Number of faucets: 6.\\n   - Rate of one faucet: \\(\\frac{100}{18}\\) gallons per minute.\\n   - Total rate for six faucets:\\n   \\[\\n   \\text{Rate of six faucets} = 6 \\times \\frac{100}{18} = \\frac{600}{18} = \\frac{100}{3} \\text{ gallons per minute}\\n   \\]\\n\\n3. **Calculate the time to fill a 25-gallon tub:**\\n   - Volume to be filled: 25 gallons.\\n   - Rate of six faucets: \\(\\frac{100}{3}\\) gallons per minute.\\n   - Time required:\\n   \\[\\n   \\text{Time} = \\frac{\\text{Volume}}{\\text{Rate}} = \\frac{25 \\text{ gallons}}{\\frac{100}{3} \\text{ gallons per minute}} = 25 \\times \\frac{3}{100} = \\frac{75}{100} = 0.75 \\text{ minutes}\\n   \\]\\n\\n4. **Convert minutes to seconds:**\\n   - 0.75 minutes = 0.75 × 60 seconds = 45 seconds.\\n\\nTherefore, it takes six faucets **45 seconds** to fill a 25-gallon tub.\n",
      "  2) reward=1.000 len=497:  To solve this problem, we need to determine the rate at which each faucet fills the tub and then use that rate to find out how long it takes for six faucets to fill a 25-gallon tub.\\n\\nFirst, let's find the rate at which one faucet fills the tub. We know that three faucets fill a 100-gallon tub in 6 minutes. Therefore, the combined rate of the three faucets is:\\n\\n\\[\\n\\text{Combined rate of three faucets} = \\frac{100 \\text{ gallons}}{6 \\text{ minutes}} = \\frac{50}{3} \\text{ gallons per minute}\\n\\]\\n\\nSince the three faucets fill the tub together, each faucet fills the tub at a rate of:\\n\\n\\[\\n\\text{Rate of one faucet} = \\frac{50}{3 \\times 3} = \\frac{50}{9} \\text{ gallons per minute}\\n\\]\\n\\nNow, we need to find out how long it takes for six faucets to fill a 25-gallon tub. The combined rate of six faucets is:\\n\\n\\[\\n\\text{Combined rate of six faucets} = 6 \\times \\frac{50}{9} = \\frac{300}{9} = \\frac{100}{3} \\text{ gallons per minute}\\n\\]\\n\\nTo find the time it takes for six faucets to fill 25 gallons, we use the formula:\\n\\n\\[\\n\\text{Time} = \\frac{\\text{Total volume}}{\\text{Rate}} = \\frac{25 \\text{ gallons}}{\\frac{100}{3} \\text{ gallons per minute}} = 25 \\times \\frac{3}{100} = \\frac{75}{100} = \\frac{3}{4} \\text{ minutes}\\n\\]\\n\\nSince we need the answer in seconds, we convert \\(\\frac{3}{4}\\) minutes to seconds:\\n\\n\\[\\n\\frac{3}{4} \\text{ minutes} = \\frac{3}{4} \\times 60 \\text{ seconds} = 45 \\text{ seconds}\\n\\]\\n\\nThus, the time it takes for six faucets to fill a 25-gallon tub is:\\n\\n\\[\\n\\boxed{45}\\n\\]\n",
      "  3) reward=1.000 len=406:  Let's solve the problem step by step.\\n\\n### **Step 1: Determine the Rate of One Faucet**\\n\\nFirst, find out how much water one faucet can fill in one minute.\\n\\n- **Given:**\\n  - Three faucets fill a 100-gallon tub in 6 minutes.\\n  \\n- **Calculation:**\\n  \\[\\n  \\text{Total water filled by one faucet in one minute} = \\frac{\\text{Total water}}{\\text{Number of faucets} \\times \\text{Time}} = \\frac{100 \\text{ gallons}}{3 \\text{ faucets} \\times 6 \\text{ minutes}} = \\frac{100}{18} \\approx 5.56 \\text{ gallons per minute}\\n  \\]\\n\\n### **Step 2: Determine the Rate of Six Faucets**\\n\\nNow, calculate how much water six faucets can fill in one minute.\\n\\n\\[\\n\\text{Total water filled by six faucets in one minute} = 6 \\text{ faucets} \\times 5.56 \\text{ gallons per minute} \\approx 33.36 \\text{ gallons per minute}\\n\\]\\n\\n### **Step 3: Calculate the Time to Fill a 25-Gallon Tub**\\n\\nFinally, find out how long it takes for six faucets to fill a 25-gallon tub.\\n\\n\\[\\n\\text{Time} = \\frac{\\text{Total water}}{\\text{Rate}} = \\frac{25 \\text{ gallons}}{33.36 \\text{ gallons per minute}} \\approx 0.75 \\text{ minutes}\\n\\]\\n\\nConvert minutes to seconds:\\n\\n\\[\\n0.75 \\text{ minutes} \\times 60 \\text{ seconds per minute} = 45 \\text{ seconds}\\n\\]\\n\\n### **Final Answer**\\n\\n\\[\\n\\boxed{45}\\n\\]\n",
      "\n",
      "Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00020.pth\n",
      "[Step 21/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=331.0\n",
      "[Step 22/50] loss=-10.3914 reward_avg=0.250 avg_resp_len=406.8\n",
      "[Step 23/50] loss=-8.8203 reward_avg=0.250 avg_resp_len=476.0\n",
      "[Step 24/50] loss=-5.8666 reward_avg=0.750 avg_resp_len=373.5\n",
      "[Step 25/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=383.5\n",
      "Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00025.pth\n",
      "[Step 26/50] loss=-3.2230 reward_avg=0.750 avg_resp_len=364.8\n",
      "[Step 27/50] loss=4.5782 reward_avg=0.750 avg_resp_len=423.0\n",
      "[Step 28/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=296.8\n",
      "[Step 29/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=469.8\n",
      "[Step 30/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=348.0\n",
      "[Step 30] sample outputs\n",
      "  1) reward=1.000 len=420:  To find the sum of the possible values of \\( x \\) that satisfy the equation \\((x+2)(x-3) = 14\\), follow these steps:\\n\\n1. **Expand the Equation:**\\n   \\n   Start by expanding the left-hand side of the equation:\\n   \\[\\n   (x+2)(x-3) = x^2 - 3x + 2x - 6 = x^2 - x - 6\\n   \\]\\n   \\n2. **Set Up the Quadratic Equation:**\\n   \\n   Set the expanded form equal to 14:\\n   \\[\\n   x^2 - x - 6 = 14\\n   \\]\\n   \\n3. **Bring All Terms to One Side:**\\n   \\n   Subtract 14 from both sides to set the equation to zero:\\n   \\[\\n   x^2 - x - 6 - 14 = 0 \\quad \\Rightarrow \\quad x^2 - x - 20 = 0\\n   \\]\\n   \\n4. **Identify the Coefficients:**\\n   \\n   The quadratic equation is in the standard form \\( ax^2 + bx + c = 0 \\), where:\\n   \\[\\n   a = 1, \\quad b = -1, \\quad c = -20\\n   \\]\\n   \\n5. **Find the Sum of the Roots:**\\n   \\n   For a quadratic equation \\( ax^2 + bx + c = 0 \\), the sum of the roots (\\( x_1 + x_2 \\)) is given by:\\n   \\[\\n   x_1 + x_2 = -\\frac{b}{a}\\n   \\]\\n   \\n   Plugging in the values:\\n   \\[\\n   x_1 + x_2 = -\\frac{-1}{1} = 1\\n   \\]\\n   \\n6. **Final Answer:**\\n   \\n   The sum of the possible values of \\( x \\) is:\\n   \\[\\n   \\boxed{1}\\n   \\]\n",
      "  2) reward=1.000 len=306:  To find the sum of the possible values of \\( x \\) that satisfy the equation \\((x+2)(x-3)=14\\), follow these steps:\\n\\n1. **Expand the Equation:**\\n   \\n   Start by expanding the left-hand side of the equation:\\n   \\[\\n   (x+2)(x-3) = x^2 - 3x + 2x - 6 = x^2 - x - 6\\n   \\]\\n   \\n   So, the equation becomes:\\n   \\[\\n   x^2 - x - 6 = 14\\n   \\]\\n\\n2. **Form a Quadratic Equation:**\\n   \\n   Move all terms to one side to form a standard quadratic equation:\\n   \\[\\n   x^2 - x - 6 - 14 = 0 \\quad \\Rightarrow \\quad x^2 - x - 20 = 0\\n   \\]\\n\\n3. **Find the Sum of the Roots:**\\n   \\n   For a quadratic equation of the form \\( ax^2 + bx + c = 0 \\), the sum of the roots is given by \\( -\\frac{b}{a} \\).\\n   \\n   Here, \\( a = 1 \\) and \\( b = -1 \\), so:\\n   \\[\\n   \\text{Sum of the roots} = -\\frac{-1}{1} = 1\\n   \\]\\n\\n**Final Answer:**\\n\\[\\n\\boxed{1}\\n\\]\n",
      "  3) reward=1.000 len=302:  To find the sum of the possible values of \\( x \\) that satisfy the equation \\((x+2)(x-3) = 14\\), follow these steps:\\n\\n1. **Expand the Left Side:**\\n   \\n   \\[\\n   (x+2)(x-3) = x^2 - 3x + 2x - 6 = x^2 - x - 6\\n   \\]\\n   \\n   So, the equation becomes:\\n   \\n   \\[\\n   x^2 - x - 6 = 14\\n   \\]\\n\\n2. **Bring All Terms to One Side:**\\n   \\n   Subtract 14 from both sides to set the equation to zero:\\n   \\n   \\[\\n   x^2 - x - 6 - 14 = 0 \\quad \\Rightarrow \\quad x^2 - x - 20 = 0\\n   \\]\\n\\n3. **Find the Sum of the Roots:**\\n   \\n   For a quadratic equation of the form \\( ax^2 + bx + c = 0 \\), the sum of the roots is given by \\( -\\frac{b}{a} \\).\\n   \\n   Here, \\( a = 1 \\) and \\( b = -1 \\), so:\\n   \\n   \\[\\n   \\text{Sum of the roots} = -\\frac{-1}{1} = 1\\n   \\]\\n\\n**Final Answer:**\\n\\n\\[\\n\\boxed{1}\\n\\]\n",
      "\n",
      "Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00030.pth\n",
      "[Step 31/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=313.5\n",
      "[Step 32/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=415.0\n",
      "[Step 33/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=444.5\n",
      "[Step 34/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=265.5\n",
      "[Step 35/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=226.0\n",
      "Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00035.pth\n",
      "[Step 36/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=491.5\n",
      "[Step 37/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=362.2\n",
      "[Step 38/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=162.8\n",
      "[Step 39/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=460.8\n",
      "[Step 40/50] loss=-25.4654 reward_avg=0.750 avg_resp_len=398.8\n",
      "[Step 40] sample outputs\n",
      "  1) reward=1.000 len=395:  To solve the problem, we need to find all integers \\( x \\) such that the square of the integer is 182 greater than the integer itself. Mathematically, this can be expressed as:\\n\\n\\[ x^2 = x + 182 \\]\\n\\nRearranging the equation, we get a standard quadratic equation:\\n\\n\\[ x^2 - x - 182 = 0 \\]\\n\\nWe will solve this quadratic equation using the quadratic formula:\\n\\n\\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\]\\n\\nwhere \\( a = 1 \\), \\( b = -1 \\), and \\( c = -182 \\).\\n\\nLet's calculate the solutions for \\( x \\) using Python.\\n```python\\nimport math\\n\\n# Coefficients of the quadratic equation\\na = 1\\nb = -1\\nc = -182\\n\\n# Calculate the discriminant\\ndiscriminant = b**2 - 4*a*c\\n\\n# Calculate the two solutions using the quadratic formula\\nx1 = (-b + math.sqrt(discriminant)) / (2*a)\\nx2 = (-b - math.sqrt(discriminant)) / (2*a)\\n\\nprint((x1, x2))\\n```\\n```output\\n(14.0, -13.0)\\n```\\nThe solutions to the quadratic equation \\( x^2 - x - 182 = 0 \\) are \\( x = 14 \\) and \\( x = -13 \\).\\n\\nTo find the sum of all integers for which the condition is true, we add these two solutions together:\\n\\n\\[ 14 + (-13) = 1 \\]\\n\\nThus, the sum of all integers for which the condition \\( x^2 = x + 182 \\) is true is:\\n\\n\\[\\n\\boxed{1}\\n\\]\n",
      "  2) reward=1.000 len=398:  Let the integer be \\( x \\). According to the problem, the square of the integer is 182 greater than the integer itself. This can be written as:\\n\\[ x^2 = x + 182 \\]\\n\\nRearrange the equation to standard quadratic form:\\n\\[ x^2 - x - 182 = 0 \\]\\n\\nWe need to solve this quadratic equation. We can use the quadratic formula, which is given by:\\n\\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\]\\nwhere \\( a = 1 \\), \\( b = -1 \\), and \\( c = -182 \\).\\n\\nFirst, calculate the discriminant:\\n\\[ \\Delta = b^2 - 4ac = (-1)^2 - 4(1)(-182) = 1 + 728 = 729 \\]\\n\\nNext, find the square root of the discriminant:\\n\\[ \\sqrt{729} = 27 \\]\\n\\nNow, substitute the values into the quadratic formula:\\n\\[ x = \\frac{-(-1) \\pm 27}{2 \\cdot 1} = \\frac{1 \\pm 27}{2} \\]\\n\\nThis gives us two solutions:\\n\\[ x = \\frac{1 + 27}{2} = \\frac{28}{2} = 14 \\]\\n\\[ x = \\frac{1 - 27}{2} = \\frac{-26}{2} = -13 \\]\\n\\nThe integers that satisfy the equation are \\( x = 14 \\) and \\( x = -13 \\). The sum of these integers is:\\n\\[ 14 + (-13) = 1 \\]\\n\\nTherefore, the sum of all integers for which the given condition is true is:\\n\\[ \\boxed{1} \\]\n",
      "  3) reward=0.000 len=512:  To solve the problem, we need to find all integers \\( x \\) such that the square of the integer is 182 greater than the integer itself. Mathematically, this can be expressed as:\\n\\n\\[ x^2 = x + 182 \\]\\n\\nRearranging the equation gives us a standard quadratic equation:\\n\\n\\[ x^2 - x - 182 = 0 \\]\\n\\nWe will solve this quadratic equation using the quadratic formula:\\n\\n\\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\]\\n\\nwhere \\( a = 1 \\), \\( b = -1 \\), and \\( c = -182 \\).\\n\\nLet's calculate the solutions using Python.\\n```python\\nimport math\\n\\n# Coefficients of the quadratic equation ax^2 + bx + c = 0\\na = 1\\nb = -1\\nc = -182\\n\\n# Calculate the discriminant\\ndiscriminant = b**2 - 4*a*c\\n\\n# Calculate the two solutions using the quadratic formula\\nx1 = (-b + math.sqrt(discriminant)) / (2*a)\\nx2 = (-b - math.sqrt(discriminant)) / (2*a)\\n\\nprint((x1, x2))\\n```\\n```output\\n(13.856824833693193, -13.856824833693193)\\n```\\nThe two solutions to the quadratic equation are approximately:\\n\\n\\[ x_1 \\approx 13.86 \\]\\n\\[ x_2 \\approx -13.86 \\]\\n\\nSince we are looking for integer solutions, we need to check the nearest integers around these solutions.\\n\\n1. For \\( x_1 \\approx 13.86 \\), the nearest integers are \\( 13 \\) and \\( 14 \\).\\n2. For \\( x_2 \\approx -13.86 \\), the nearest integers are \\( -14 \\) and \\( -13 \\).\\n\\nWe will now verify which of these integer solutions satisfy the original equation \\( x^2 = x + 182 \\).\\n\\n### Checking \\( x = 13 \\):\\n\\n\\[ 13^2 = 13 + 182 \\]\\n\\[ 169 = 195 \\]\\n\\nThis is not true, so \\(\n",
      "\n",
      "Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00040.pth\n",
      "[Step 41/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=303.2\n",
      "[Step 42/50] loss=-7.8943 reward_avg=0.500 avg_resp_len=383.5\n",
      "[Step 43/50] loss=-7.7182 reward_avg=0.500 avg_resp_len=406.5\n",
      "[Step 44/50] loss=-1.9661 reward_avg=0.250 avg_resp_len=504.8\n",
      "[Step 45/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=224.2\n",
      "Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00045.pth\n",
      "[Step 46/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=264.5\n",
      "[Step 47/50] loss=-8.2894 reward_avg=0.500 avg_resp_len=483.8\n",
      "[Step 48/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=387.0\n",
      "[Step 49/50] loss=10.1916 reward_avg=0.250 avg_resp_len=509.8\n",
      "[Step 50/50] loss=-10.8985 reward_avg=0.250 avg_resp_len=332.2\n",
      "[Step 50] sample outputs\n",
      "  1) reward=0.000 len=236:  To factor the expression \\(58x^5 - 203x^{11}\\), follow these steps:\\n\\n1. **Identify the Greatest Common Factor (GCF):**\\n   \\n   - **Coefficients:** The GCF of 58 and 203 is **17**.\\n   - **Variables:** The smallest power of \\(x\\) present in both terms is \\(x^5\\).\\n   \\n   Therefore, the GCF of the expression is \\(17x^5\\).\\n\\n2. **Factor out the GCF:**\\n   \\n   \\[\\n   58x^5 - 203x^{11} = 17x^5 (4 - 11x^6)\\n   \\]\\n\\n3. **Simplify the Expression Inside the Parentheses:**\\n   \\n   \\[\\n   4 - 11x^6\\n   \\]\\n   \\n   This expression cannot be factored further using real numbers.\\n\\n4. **Final Factored Form:**\\n   \\n   \\[\\n   \\boxed{17x^5(4 - 11x^6)}\\n   \\]\n",
      "  2) reward=0.000 len=385:  To factor the expression \\(58x^5 - 203x^{11}\\), follow these steps:\\n\\n1. **Identify the Greatest Common Factor (GCF):**\\n   \\n   - **Coefficients:** The coefficients are 58 and 203. The GCF of 58 and 203 is **17**.\\n   - **Variables:** The terms have \\(x^5\\) and \\(x^{11}\\). The smallest exponent is 5, so the GCF for the variable part is \\(x^5\\).\\n   \\n   Therefore, the GCF of the entire expression is \\(17x^5\\).\\n\\n2. **Factor Out the GCF:**\\n   \\n   \\[\\n   58x^5 - 203x^{11} = 17x^5 (4 - 11x^6)\\n   \\]\\n\\n3. **Check if the Expression Inside the Parentheses is Factored Further:**\\n   \\n   The expression inside the parentheses, \\(4 - 11x^6\\), is a difference of squares. It can be factored as:\\n   \\n   \\[\\n   4 - 11x^6 = (2)^2 - (x^3)^2 = (2 - x^3)(2 + x^3)\\n   \\]\\n   \\n   However, \\(2 + x^3\\) does not factor further over the real numbers.\\n\\n4. **Final Factored Form:**\\n   \\n   \\[\\n   58x^5 - 203x^{11} = 17x^5 (2 - x^3)(2 + x^3)\\n   \\]\\n\\n**Final Answer:**\\n\\n\\[\\n\\boxed{17x^5 (2 - x^3)(2 + x^3)}\\n\\]\n",
      "  3) reward=0.000 len=512:  To factor the expression \\( 58x^5 - 203x^{11} \\), follow these steps:\\n\\n1. **Identify the Greatest Common Factor (GCF):**\\n   \\n   - **Coefficients:** The coefficients are 58 and 203. The GCF of 58 and 203 is **17**.\\n   - **Variables:** The terms have \\( x^5 \\) and \\( x^{11} \\). The GCF for the variables is \\( x^5 \\).\\n\\n   Therefore, the GCF of the entire expression is \\( 17x^5 \\).\\n\\n2. **Factor Out the GCF:**\\n   \\n   \\[\\n   58x^5 - 203x^{11} = 17x^5 \\left( \\frac{58x^5}{17x^5} - \\frac{203x^{11}}{17x^5} \\right)\\n   \\]\\n   \\n   Simplify the terms inside the parentheses:\\n   \\n   \\[\\n   = 17x^5 \\left( 3.5 - 11.935x^6 \\right)\\n   \\]\\n   \\n   However, let's approach this differently for clarity.\\n\\n3. **Factor by Grouping:**\\n   \\n   \\[\\n   58x^5 - 203x^{11} = -203x^{11} + 58x^5\\n   \\]\\n   \\n   Factor out the common terms:\\n   \\n   \\[\\n   = -x^5 (203x^6 - 58)\\n   \\]\\n   \\n   Simplify the expression inside the parentheses:\\n   \\n   \\[\\n   = -x^5 (203x^6 - 58) = -x^5 (203x^6 - 58)\\n   \\]\\n   \\n   However, let's find a common factor for both terms in the parentheses.\\n\\n4. **Find a Common Factor:**\\n   \\n   Notice that \\( 203x^6 - 58 \\) can be factored as:\\n   \\n   \\[\\n   203x^6 - 58 = 58(3.5x^6 - 1)\\n   \\]\\n   \\n   Since \\( 3.5x^6 - 1 \\) cannot be fact\n",
      "\n",
      "Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00050.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3Model(\n",
       "  (tok_emb): Embedding(151936, 1024)\n",
       "  (trf_blocks): ModuleList(\n",
       "    (0-27): 28 x TransformerBlock(\n",
       "      (att): GroupedQueryAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc2): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc3): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "train_rlvr_grpo(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    math_data=math_train,\n",
    "    device=device,\n",
    "    steps=50,\n",
    "    num_rollouts=4,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    lr=1e-5,\n",
    "    checkpoint_every=5,\n",
    "    checkpoint_dir=\".\",\n",
    "    csv_log_path=\"train_rlvr_grpo_metrics.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbcdd3f",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5477cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_math500_test(local_path=\"math500_test.json\", save_copy=True):\n",
    "    local_path = Path(local_path)\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/reasoning-from-scratch/\"\n",
    "        \"main/ch03/01_main-chapter-code/math500_test.json\"\n",
    "    )\n",
    "\n",
    "    if local_path.exists():\n",
    "        with local_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "\n",
    "        if save_copy:  # Saves a local copy\n",
    "            with local_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "\n",
    "    return data\n",
    "\n",
    "math_data = load_math500_test()\n",
    "len(math_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e743a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_text_basic_stream_cache(\n",
    "    model,\n",
    "    token_ids,\n",
    "    max_new_tokens,\n",
    "    eos_token_id=None\n",
    "):\n",
    "    # input_length = token_ids.shape[1]\n",
    "    model.eval()\n",
    "    cache = KVCache(n_layers=model.cfg[\"n_layers\"])\n",
    "    model.reset_kv_cache()\n",
    "\n",
    "    out = model(token_ids, cache=cache)[:, -1]\n",
    "    for _ in range(max_new_tokens):\n",
    "        next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
    "\n",
    "        if (eos_token_id is not None\n",
    "                and torch.all(next_token == eos_token_id)):\n",
    "            break\n",
    "\n",
    "        yield next_token  # New: Yield each token as it's generated\n",
    "        # token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "        out = model(next_token, cache=cache)[:, -1]\n",
    "\n",
    "    # return token_ids[:, input_length:]\n",
    "\n",
    "def generate_text_stream_concat(\n",
    "    model, tokenizer, prompt, device, max_new_tokens,\n",
    "    verbose=False,\n",
    "):\n",
    "    input_ids = torch.tensor(\n",
    "        tokenizer.encode(prompt), device=device\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "    generated_ids = []\n",
    "    for token in generate_text_basic_stream_cache(\n",
    "        model=model,\n",
    "        token_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    ):\n",
    "        next_token_id = token.squeeze(0)\n",
    "        generated_ids.append(next_token_id.item())\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                tokenizer.decode(next_token_id.tolist()),\n",
    "                end=\"\",\n",
    "                flush=True\n",
    "            )\n",
    "    return tokenizer.decode(generated_ids)\n",
    "\n",
    "def eta_progress_message(\n",
    "    processed,\n",
    "    total,\n",
    "    start_time,\n",
    "    show_eta=False,\n",
    "    label=\"Progress\",\n",
    "):\n",
    "    progress = f\"{label}: {processed}/{total}\"\n",
    "    pad_width = len(f\"{label}: {total}/{total} | ETA: 00h 00m 00s\")\n",
    "    if not show_eta or processed <= 0:\n",
    "        return progress.ljust(pad_width)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    if elapsed <= 0:\n",
    "        return progress.ljust(pad_width)\n",
    "\n",
    "    remaining = max(total - processed, 0)\n",
    "\n",
    "    if processed:\n",
    "        avg_time = elapsed / processed\n",
    "        eta_seconds = avg_time * remaining\n",
    "    else:\n",
    "        eta_seconds = 0\n",
    "\n",
    "    eta_seconds = max(int(round(eta_seconds)), 0)\n",
    "    minutes, rem_seconds = divmod(eta_seconds, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    if hours:\n",
    "        eta = f\"{hours}h {minutes:02d}m {rem_seconds:02d}s\"\n",
    "    elif minutes:\n",
    "        eta = f\"{minutes:02d}m {rem_seconds:02d}s\"\n",
    "    else:\n",
    "        eta = f\"{rem_seconds:02d}s\"\n",
    "\n",
    "    message = f\"{progress} | ETA: {eta}\"\n",
    "    return message.ljust(pad_width)\n",
    "\n",
    "def evaluate_math500_stream(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    math_data,\n",
    "    out_path=None,\n",
    "    max_new_tokens=512,\n",
    "    verbose=False,\n",
    "):\n",
    "\n",
    "    if out_path is None:\n",
    "        dev_name = str(device).replace(\":\", \"-\")  # Make filename compatible with Windows\n",
    "        out_path = Path(f\"math500-{dev_name}.jsonl\")\n",
    "\n",
    "    num_examples = len(math_data)\n",
    "    num_correct = 0\n",
    "    total_len = 0  # Calculates the average response length (see exercise 3.2)\n",
    "    start_time = time.time()\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:  # Save results for inspection\n",
    "        for i, row in enumerate(math_data, start=1):\n",
    "            prompt = render_prompt(row[\"problem\"])    # 1. Apply prompt template\n",
    "            gen_text = generate_text_stream_concat(   # 2. Generate response\n",
    "                model, tokenizer, prompt, device,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "            total_len += len(tokenizer.encode(gen_text))\n",
    "\n",
    "            extracted = extract_final_candidate(  # 3. Extract and normalize answer\n",
    "                gen_text\n",
    "            )\n",
    "            is_correct = grade_answer(            # 4. Grade answer\n",
    "                extracted, row[\"answer\"]\n",
    "            )\n",
    "            num_correct += int(is_correct)\n",
    "\n",
    "            record = {  # Record to be saved for inspection\n",
    "                \"index\": i,\n",
    "                \"problem\": row[\"problem\"],\n",
    "                \"gtruth_answer\": row[\"answer\"],\n",
    "                \"generated_text\": gen_text,\n",
    "                \"extracted\": extracted,\n",
    "                \"correct\": bool(is_correct),\n",
    "            }\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            progress_msg = eta_progress_message(\n",
    "                processed=i,\n",
    "                total=num_examples,\n",
    "                start_time=start_time,\n",
    "                show_eta=True,\n",
    "                label=\"MATH-500\",\n",
    "            )\n",
    "            print(progress_msg, end=\"\\r\", flush=True)\n",
    "            if verbose:  # Print responses during the generation process\n",
    "                print(\n",
    "                    f\"\\n\\n{'='*50}\\n{progress_msg}\\n\"\n",
    "                    f\"{'='*50}\\nExtracted: {extracted}\\n\"\n",
    "                    f\"Expected:  {row['answer']}\\n\"\n",
    "                    f\"Correct so far: {num_correct}\\n{'-'*50}\"\n",
    "                )\n",
    "\n",
    "    # Print summary information\n",
    "    seconds_elapsed = time.time() - start_time\n",
    "    acc = num_correct / num_examples if num_examples else 0.0\n",
    "    print(f\"\\nAccuracy: {acc*100:.1f}% ({num_correct}/{num_examples})\")\n",
    "    print(f\"Total time: {seconds_elapsed/60:.1f} min\")\n",
    "    avg_len = total_len / num_examples\n",
    "    print(f\"Average response length: {avg_len:.2f} tokens\")\n",
    "    print(f\"Logs written to: {out_path}\")\n",
    "    return num_correct, num_examples, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fff04070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATH-500: 10/10 | ETA: 00s        \n",
      "Accuracy: 60.0% (6/10)\n",
      "Total time: 2.2 min\n",
      "Average response length: 759.20 tokens\n",
      "Logs written to: math500_step00050-evaluate-script.jsonl\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = 'qwen3-0.6B-rlvr-grpo-step00050.pth'\n",
    "which_model = checkpoint_path.split('.')[1].split('-')[-1]\n",
    "model = Qwen3Model(QWEN_CONFIG_06_B)\n",
    "state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "num_correct, num_examples, acc = evaluate_math500_stream(\n",
    "    model=model,\n",
    "    out_path=f\"math500_{which_model}-evaluate-script.jsonl\",\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    math_data=math_data[:10],\n",
    "    max_new_tokens=2048,\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acfa98ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATH-500: 10/10 | ETA: 00s        \n",
      "Accuracy: 30.0% (3/10)\n",
      "Total time: 0.3 min\n",
      "Average response length: 95.60 tokens\n",
      "Logs written to: math500_base-evaluate-script.jsonl\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = 'qwen3/qwen3-0.6B-base.pth'\n",
    "which_model = checkpoint_path.split('.')[1].split('-')[-1]\n",
    "model = Qwen3Model(QWEN_CONFIG_06_B)\n",
    "state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "num_correct, num_examples, acc = evaluate_math500_stream(\n",
    "    model=model,\n",
    "    out_path=f\"math500_{which_model}-evaluate-script.jsonl\",\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    math_data=math_data[:10],\n",
    "    max_new_tokens=2048,\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f3234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
