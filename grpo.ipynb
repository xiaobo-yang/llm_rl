{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10b3ed2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# GRPO from scratch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6286e1",
   "metadata": {},
   "source": [
    "## algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20911f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ccca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def render_prompt(prompt):\n",
    "    template = (\n",
    "        \"You are a helpful math assistant.\\n\"\n",
    "        \"Answer the question and write the final result on a new line as:\\n\"\n",
    "        \"\\\\boxed{ANSWER}\\n\\n\"\n",
    "        f\"Question:\\n{prompt}\\n\\nAnswer:\"\n",
    "    )\n",
    "    return template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7801ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    def __init__(self, n_layers):\n",
    "        self.cache = [None] * n_layers\n",
    "\n",
    "    def get(self, layer_idx):\n",
    "        return self.cache[layer_idx]\n",
    "\n",
    "    def update(self, layer_idx, value):\n",
    "        self.cache[layer_idx] = value\n",
    "\n",
    "    def get_all(self):\n",
    "        return self.cache\n",
    "\n",
    "    def reset(self):\n",
    "        for i in range(len(self.cache)):\n",
    "            self.cache[i] = None\n",
    "\n",
    "def top_p_filter(probas, top_p):\n",
    "    if top_p is None or top_p >= 1.0:\n",
    "        return probas\n",
    "\n",
    "    # Step 4.1: Sort by descending probability\n",
    "    sorted_probas, sorted_idx = torch.sort(probas, dim=1, descending=True)\n",
    "\n",
    "    # Step 4.2: Cumulative sum\n",
    "    cumprobas = torch.cumsum(sorted_probas, dim=1)\n",
    "\n",
    "    # Step 4.3.1: Keep tokens where prefix cumulative mass (before token) is < top_ps\n",
    "    # Example: [0.5, 0.41, 0.09] with top_p=0.9 should keep the first two tokens\n",
    "    prefix = cumprobas - sorted_probas   # cumulative mass before each token\n",
    "    keep = prefix < top_p\n",
    "    # Always keep at least one token (fallback for very small/non-positive top_p)\n",
    "    keep[:, 0] = True\n",
    "\n",
    "    # Step 4.3.2: Zero out beyond cutoff\n",
    "    kept_sorted = torch.where(\n",
    "        keep, sorted_probas,\n",
    "        torch.zeros_like(sorted_probas)\n",
    "    )\n",
    "    # Step 4.3.3: Map back to original order\n",
    "    filtered = torch.zeros_like(probas).scatter(1, sorted_idx, kept_sorted)\n",
    "\n",
    "    # Step 4.4: Renormalize to sum to 1\n",
    "    denom = torch.sum(filtered, dim=1).clamp_min(1e-12)\n",
    "    return filtered / denom\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_response(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    device,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "):\n",
    "    input_ids = torch.tensor(\n",
    "        tokenizer.encode(prompt),\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "    cache = KVCache(n_layers=model.cfg[\"n_layers\"])\n",
    "    model.reset_kv_cache()\n",
    "    logits = model(input_ids.unsqueeze(0), cache=cache)[:, -1]\n",
    "\n",
    "    generated = []\n",
    "    for _ in range(max_new_tokens):\n",
    "        if temperature and temperature != 1.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        probas = top_p_filter(probas, top_p)\n",
    "        next_token = torch.multinomial(\n",
    "            probas.cpu(), num_samples=1\n",
    "        ).to(device)\n",
    "\n",
    "        if (\n",
    "            tokenizer.eos_token_id is not None\n",
    "            and next_token.item() == tokenizer.eos_token_id\n",
    "        ):\n",
    "            break\n",
    "        generated.append(next_token.item())\n",
    "        logits = model(next_token, cache=cache)[:, -1]\n",
    "\n",
    "    full_token_ids = torch.cat(\n",
    "        [input_ids,\n",
    "         torch.tensor(generated, device=device, dtype=input_ids.dtype),]\n",
    "    )\n",
    "    return full_token_ids, input_ids.numel(), tokenizer.decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31725a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sympy import simplify\n",
    "from sympy.parsing import sympy_parser as spp\n",
    "from sympy.core.sympify import SympifyError\n",
    "from sympy.polys.polyerrors import PolynomialError\n",
    "from tokenize import TokenError\n",
    "\n",
    "RE_NUMBER = re.compile(\n",
    "    r\"-?(?:\\d+/\\d+|\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?)\"\n",
    ")\n",
    "\n",
    "LATEX_FIXES = [  # Latex formatting to be replaced\n",
    "    (r\"\\\\left\\s*\", \"\"),\n",
    "    (r\"\\\\right\\s*\", \"\"),\n",
    "    (r\"\\\\,|\\\\!|\\\\;|\\\\:\", \"\"),\n",
    "    (r\"\\\\cdot\", \"*\"),\n",
    "    (r\"\\u00B7|\\u00D7\", \"*\"),\n",
    "    (r\"\\\\\\^\\\\circ\", \"\"),\n",
    "    (r\"\\\\dfrac\", r\"\\\\frac\"),\n",
    "    (r\"\\\\tfrac\", r\"\\\\frac\"),\n",
    "    (r\"°\", \"\"),\n",
    "]\n",
    "\n",
    "RE_SPECIAL = re.compile(r\"<\\|[^>]+?\\|>\")  # strip chat special tokens like <|assistant|>\n",
    "SUPERSCRIPT_MAP = {\n",
    "    \"⁰\": \"0\", \"¹\": \"1\", \"²\": \"2\", \"³\": \"3\", \"⁴\": \"4\",\n",
    "    \"⁵\": \"5\", \"⁶\": \"6\", \"⁷\": \"7\", \"⁸\": \"8\", \"⁹\": \"9\",\n",
    "    \"⁺\": \"+\", \"⁻\": \"-\", \"⁽\": \"(\", \"⁾\": \")\",\n",
    "}\n",
    "\n",
    "def get_last_boxed(text):\n",
    "    # Find the last occurrence of \"\\boxed\"\n",
    "    boxed_start_idx = text.rfind(r\"\\boxed\")\n",
    "    if boxed_start_idx == -1:\n",
    "        return None\n",
    "\n",
    "    # Get position after \"\\boxed\"\n",
    "    current_idx = boxed_start_idx + len(r\"\\boxed\")\n",
    "\n",
    "    # Skip any whitespace after \"\\boxed\"\n",
    "    while current_idx < len(text) and text[current_idx].isspace():\n",
    "        current_idx += 1\n",
    "\n",
    "    # Expect an opening brace \"{\"\n",
    "    if current_idx >= len(text) or text[current_idx] != \"{\":\n",
    "        return None\n",
    "\n",
    "    # Parse the braces with nesting\n",
    "    current_idx += 1\n",
    "    brace_depth = 1\n",
    "    content_start_idx = current_idx\n",
    "\n",
    "    while current_idx < len(text) and brace_depth > 0:\n",
    "        char = text[current_idx]\n",
    "        if char == \"{\":\n",
    "            brace_depth += 1\n",
    "        elif char == \"}\":\n",
    "            brace_depth -= 1\n",
    "        current_idx += 1\n",
    "\n",
    "    # Account for unbalanced braces\n",
    "    if brace_depth != 0:\n",
    "        return None\n",
    "\n",
    "    # Extract content inside the outermost braces\n",
    "    return text[content_start_idx:current_idx-1]\n",
    "\n",
    "def extract_final_candidate(text, fallback=\"number_then_full\"):\n",
    "    # Default return value if nothing matches\n",
    "    result = \"\"\n",
    "\n",
    "    if text:\n",
    "        # Prefer the last boxed expression if present\n",
    "        boxed = get_last_boxed(text.strip())\n",
    "        if boxed:\n",
    "            result = boxed.strip().strip(\"$ \")\n",
    "\n",
    "        # If no boxed expression, try fallback\n",
    "        elif fallback in (\"number_then_full\", \"number_only\"):\n",
    "            m = RE_NUMBER.findall(text)\n",
    "            if m:\n",
    "                # Use last number\n",
    "                result = m[-1]\n",
    "            elif fallback == \"number_then_full\":\n",
    "                # Else return full text if no number found\n",
    "                result = text\n",
    "    return result\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = RE_SPECIAL.sub(\"\", text).strip()\n",
    "\n",
    "    # Strip leading multiple-choice labels\n",
    "    # E.g., like \"c. 3\" -> 3, or \"b: 2\" -> 2\n",
    "    match = re.match(r\"^[A-Za-z]\\s*[.:]\\s*(.+)$\", text)\n",
    "    if match:\n",
    "        text = match.group(1)\n",
    "\n",
    "    # Remove angle-degree markers\n",
    "    text = re.sub(r\"\\^\\s*\\{\\s*\\\\circ\\s*\\}\", \"\", text)   # ^{\\circ}\n",
    "    text = re.sub(r\"\\^\\s*\\\\circ\", \"\", text)             # ^\\circ\n",
    "    text = text.replace(\"°\", \"\")                        # Unicode degree\n",
    "\n",
    "    # unwrap \\text{...} if the whole string is wrapped\n",
    "    match = re.match(r\"^\\\\text\\{(?P<x>.+?)\\}$\", text)\n",
    "    if match:\n",
    "        text = match.group(\"x\")\n",
    "\n",
    "    # strip inline/display math wrappers \\( \\) \\[ \\]\n",
    "    text = re.sub(r\"\\\\\\(|\\\\\\)|\\\\\\[|\\\\\\]\", \"\", text)\n",
    "\n",
    "    # light LaTeX canonicalization\n",
    "    for pat, rep in LATEX_FIXES:\n",
    "        text = re.sub(pat, rep, text)\n",
    "\n",
    "    # convert unicode superscripts into exponent form (e.g., 2² -> 2**2)\n",
    "    def convert_superscripts(s, base=None):\n",
    "        converted = \"\".join(\n",
    "            SUPERSCRIPT_MAP[ch] if ch in SUPERSCRIPT_MAP else ch\n",
    "            for ch in s\n",
    "        )\n",
    "        if base is None:\n",
    "            return converted\n",
    "        return f\"{base}**{converted}\"\n",
    "\n",
    "    text = re.sub(\n",
    "        r\"([0-9A-Za-z\\)\\]\\}])([⁰¹²³⁴⁵⁶⁷⁸⁹⁺⁻]+)\",\n",
    "        lambda m: convert_superscripts(m.group(2), base=m.group(1)),\n",
    "        text,\n",
    "    )\n",
    "    text = convert_superscripts(text)\n",
    "\n",
    "    # numbers/roots\n",
    "    text = text.replace(\"\\\\%\", \"%\").replace(\"$\", \"\").replace(\"%\", \"\")\n",
    "    text = re.sub(\n",
    "        r\"\\\\sqrt\\s*\\{([^}]*)\\}\",\n",
    "        lambda match: f\"sqrt({match.group(1)})\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"\\\\sqrt\\s+([^\\\\\\s{}]+)\",\n",
    "        lambda match: f\"sqrt({match.group(1)})\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    # fractions\n",
    "    text = re.sub(\n",
    "        r\"\\\\frac\\s*\\{([^{}]+)\\}\\s*\\{([^{}]+)\\}\",\n",
    "        lambda match: f\"({match.group(1)})/({match.group(2)})\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"\\\\frac\\s+([^\\s{}]+)\\s+([^\\s{}]+)\",\n",
    "        lambda match: f\"({match.group(1)})/({match.group(2)})\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    # exponent and mixed numbers\n",
    "    text = text.replace(\"^\", \"**\")\n",
    "    text = re.sub(\n",
    "        r\"(?<=\\d)\\s+(\\d+/\\d+)\",\n",
    "        lambda match: \"+\" + match.group(1),\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    # 1,234 -> 1234\n",
    "    text = re.sub(\n",
    "        r\"(?<=\\d),(?=\\d\\d\\d(\\D|$))\",\n",
    "        \"\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    return text.replace(\"{\", \"\").replace(\"}\", \"\").strip().lower()\n",
    "\n",
    "\n",
    "def sympy_parser(expr):\n",
    "    # To avoid crashing on long garbage responses\n",
    "    # that some badly trained models (chapter 6) may emit\n",
    "    if expr is None or len(expr) > 2000:\n",
    "        return None\n",
    "    try:\n",
    "        return spp.parse_expr(\n",
    "            expr,\n",
    "            transformations=(\n",
    "                # Standard transformations like handling parentheses\n",
    "                *spp.standard_transformations,\n",
    "\n",
    "                # Allow omitted multiplication symbols (e.g., \"2x\" -> 2*x\")\n",
    "                spp.implicit_multiplication_application,\n",
    "            ),\n",
    "\n",
    "            # Evaluate during parsing so simple constants simplify (e.g., 2+3 -> 5)\n",
    "            evaluate=True,\n",
    "        )\n",
    "    except (SympifyError, SyntaxError, TypeError, AttributeError,\n",
    "            IndexError, TokenError, ValueError, PolynomialError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def equality_check(expr_gtruth, expr_pred):\n",
    "    # First, check if the two expressions are exactly the same string\n",
    "    if expr_gtruth == expr_pred:\n",
    "        return True\n",
    "\n",
    "    # Parse both expressions into SymPy objects (returns None if parsing fails)\n",
    "    gtruth, pred = sympy_parser(expr_gtruth), sympy_parser(expr_pred)\n",
    "\n",
    "    # If both expressions were parsed successfully, try symbolic comparison\n",
    "    if gtruth is not None and pred is not None:\n",
    "        try:\n",
    "            # If the difference is 0, they are equivalent\n",
    "            return simplify(gtruth - pred) == 0\n",
    "        except (SympifyError, TypeError):\n",
    "            pass\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def split_into_parts(text):\n",
    "    result = [text]\n",
    "\n",
    "    if text:\n",
    "        # Check if text looks like a tuple or list, e.g. \"(a, b)\" or \"[a, b]\"\n",
    "        if (\n",
    "            len(text) >= 2\n",
    "            and text[0] in \"([\" and text[-1] in \")]\"\n",
    "            and \",\" in text[1:-1]\n",
    "        ):\n",
    "            # Split on commas inside brackets and strip whitespace\n",
    "            items = [p.strip() for p in text[1:-1].split(\",\")]\n",
    "            if all(items):\n",
    "                result = items\n",
    "    else:\n",
    "        # If text is empty, return an empty list\n",
    "        result = []\n",
    "\n",
    "    return result\n",
    "\n",
    "def grade_answer(pred_text, gt_text):\n",
    "    result = False  # Default outcome if checks fail\n",
    "\n",
    "    # Only continue if both inputs are non-empty strings\n",
    "    if pred_text is not None and gt_text is not None:\n",
    "        gt_parts = split_into_parts(\n",
    "            normalize_text(gt_text)\n",
    "        )  # Break ground truth into comparable parts\n",
    "\n",
    "        pred_parts = split_into_parts(\n",
    "            normalize_text(pred_text)\n",
    "        )  # Break prediction into comparable parts\n",
    "\n",
    "        # Ensure both sides have same number of valid parts\n",
    "        if (gt_parts and pred_parts\n",
    "           and len(gt_parts) == len(pred_parts)):\n",
    "            result = all(\n",
    "                equality_check(gt, pred)\n",
    "                for gt, pred in zip(gt_parts, pred_parts)\n",
    "            )  # Check each part for mathematical equivalence\n",
    "\n",
    "    return result  # True only if all checks passed\n",
    "\n",
    "\n",
    "def reward_rlvr(answer_text, ground_truth):\n",
    "    extracted = extract_final_candidate(\n",
    "        answer_text, fallback=None  # Require \\boxed{}\n",
    "    )\n",
    "    if not extracted:\n",
    "        return 0.0\n",
    "    correct = grade_answer(extracted, ground_truth)\n",
    "    return float(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5cba415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(model, token_ids, prompt_len):\n",
    "    logits = model(token_ids.unsqueeze(0)).squeeze(0).float()\n",
    "    logprobs = torch.log_softmax(logits, dim=-1)\n",
    "    selected = logprobs[:-1].gather(\n",
    "        1, token_ids[1:].unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "    return torch.sum(selected[prompt_len - 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8789d",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "$$\n",
    "\\mathscr{L}(\\theta) = \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afe9af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grpo_loss(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    example,\n",
    "    device,\n",
    "    num_rollouts=2,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "):\n",
    "    assert num_rollouts >= 2\n",
    "    roll_logps, roll_rewards, samples = [], [], []\n",
    "    prompt = render_prompt(example[\"problem\"])\n",
    "\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    for _ in range(num_rollouts):\n",
    "        # Stage 1: generate rollouts\n",
    "        token_ids, prompt_len, text = sample_response(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            device=device,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        # Stage 2: compute rewards\n",
    "        reward = reward_rlvr(text, example[\"answer\"])\n",
    "        \n",
    "        # Stage 4: compute logprobs\n",
    "        logp = sequence_logprob(model, token_ids, prompt_len)\n",
    "\n",
    "        roll_logps.append(logp)\n",
    "        roll_rewards.append(reward)\n",
    "        samples.append(\n",
    "            {\n",
    "                \"text\": text,\n",
    "                \"reward\": reward,\n",
    "                \"gen_len\": token_ids.numel() - prompt_len,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    # Stage 2: collect all rewards\n",
    "    rewards = torch.tensor(roll_rewards, device=device)\n",
    "\n",
    "    # Stage 3: compute advantages\n",
    "    advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-4)\n",
    "\n",
    "    # Stage 4: collect all logprobs\n",
    "    logps = torch.stack(roll_logps)\n",
    "\n",
    "    # Stage 5: compute policy gradient loss\n",
    "    pg_loss = -(advantages.detach() * logps).mean()\n",
    "    loss = pg_loss  # In the next chapter we add a KL term here\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss.item(),\n",
    "        \"pg_loss\": pg_loss.item(),\n",
    "        \"rewards\": roll_rewards,\n",
    "        \"advantages\": advantages.detach().cpu().tolist(),\n",
    "        \"samples\": samples,\n",
    "        \"loss_tensor\": loss,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268310bf",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f0762f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def train_rlvr_grpo(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    math_data,\n",
    "    device,\n",
    "    steps=None,\n",
    "    num_rollouts=2,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    lr=1e-5,\n",
    "    checkpoint_every=50,\n",
    "    checkpoint_dir=\".\",\n",
    "    csv_log_path=None,\n",
    "\n",
    "):\n",
    "    if steps is None:\n",
    "        steps = len(math_data)\n",
    "\n",
    "    # Stage 1: initialize optimize\n",
    "    # (the model was already initialized outside the function)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    current_step = 0\n",
    "    if csv_log_path is None:\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        csv_log_path = f\"train_rlvr_grpo_metrics_{timestamp}.csv\"\n",
    "    csv_log_path = Path(csv_log_path)\n",
    "\n",
    "    try:\n",
    "        # Stage 2: Iterate over training steps\n",
    "        for step in range(steps):\n",
    "\n",
    "            # Stage 3: Reset loss gradient\n",
    "            # (it's best practice to do this at the beginning of each step)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_step = step + 1\n",
    "            example = math_data[step % len(math_data)]\n",
    "\n",
    "            # Stage 4: calculate GRPO loss\n",
    "            stats = compute_grpo_loss(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                example=example,\n",
    "                device=device,\n",
    "                num_rollouts=num_rollouts,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "            )\n",
    "\n",
    "            # Stage 5: Backward pass to calculate loss gradients\n",
    "            stats[\"loss_tensor\"].backward()\n",
    "\n",
    "            # Clip large gradients to improve training stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Stage 6: Update model weights using loss gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Stage 7: Collect rewards, response lengths, and losses\n",
    "            reward_avg = torch.tensor(stats[\"rewards\"]).mean().item()\n",
    "            step_tokens = sum(\n",
    "                sample[\"gen_len\"] for sample in stats[\"samples\"]\n",
    "            )\n",
    "            avg_response_len = (\n",
    "                step_tokens / len(stats[\"samples\"]) \n",
    "                if stats[\"samples\"] else 0.0\n",
    "            )\n",
    "            append_csv_metrics(\n",
    "                csv_log_path, current_step, steps, stats[\"loss\"],\n",
    "                reward_avg, avg_response_len,\n",
    "            )\n",
    "\n",
    "            # Print step metrics\n",
    "            print(\n",
    "                f\"[Step {current_step}/{steps}] \"\n",
    "                f\"loss={stats['loss']:.4f} \"\n",
    "                f\"reward_avg={reward_avg:.3f} \"\n",
    "                f\"avg_resp_len={avg_response_len:.1f}\"\n",
    "            )\n",
    "\n",
    "            # Sample outputs (every 10 steps) to check if model\n",
    "            # generates coherent text\n",
    "            if current_step % 10 == 0:\n",
    "                print(f\"[Step {current_step}] sample outputs\")\n",
    "                for i, sample in enumerate(stats[\"samples\"][:3]):\n",
    "                    text = sample[\"text\"].replace(\"\\n\", \"\\\\n\")\n",
    "                    print(\n",
    "                        f\"  {i+1}) reward={sample['reward']:.3f} \"\n",
    "                        f\"len={sample['gen_len']}: {text}\"\n",
    "                    )\n",
    "                print()\n",
    "\n",
    "            # Stage 8: Save model checkpoint\n",
    "            if checkpoint_every and current_step % checkpoint_every == 0:\n",
    "                ckpt_path = save_checkpoint(\n",
    "                    model=model,\n",
    "                    checkpoint_dir=checkpoint_dir,\n",
    "                    step=current_step,\n",
    "                )\n",
    "                print(f\"Saved checkpoint to {ckpt_path}\")\n",
    "\n",
    "    # Save a model checkpoint if we interrupt the training early\n",
    "    except KeyboardInterrupt:\n",
    "        ckpt_path = save_checkpoint(\n",
    "            model=model,\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            step=max(1, current_step),\n",
    "            suffix=\"interrupt\",\n",
    "        )\n",
    "        print(f\"\\nKeyboardInterrupt. Saved checkpoint to {ckpt_path}\")\n",
    "        return model\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_checkpoint(model, checkpoint_dir, step, suffix=\"\"):\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    suffix = f\"-{suffix}\" if suffix else \"\"\n",
    "    ckpt_path = (\n",
    "        checkpoint_dir /\n",
    "        f\"qwen3-0.6B-rlvr-grpo-step{step:05d}{suffix}.pth\"\n",
    "    )\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "    return ckpt_path\n",
    "\n",
    "\n",
    "def append_csv_metrics(\n",
    "    csv_log_path,\n",
    "    step_idx,\n",
    "    total_steps,\n",
    "    loss,\n",
    "    reward_avg,\n",
    "    avg_response_len,\n",
    "):\n",
    "    if not csv_log_path.exists():\n",
    "        csv_log_path.write_text(\n",
    "            \"step,total_steps,loss,reward_avg,avg_response_len\\n\",\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "    with csv_log_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\n",
    "            f\"{step_idx},{total_steps},{loss:.6f},{reward_avg:.6f},\"\n",
    "            f\"{avg_response_len:.6f}\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96e64a",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a92a1",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14d8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3Tokenizer:\n",
    "    _SPECIALS = [\n",
    "        \"<|endoftext|>\",\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|object_ref_start|>\", \"<|object_ref_end|>\",\n",
    "        \"<|box_start|>\", \"<|box_end|>\",\n",
    "        \"<|quad_start|>\", \"<|quad_end|>\",\n",
    "        \"<|vision_start|>\", \"<|vision_end|>\",\n",
    "        \"<|vision_pad|>\", \"<|image_pad|>\", \"<|video_pad|>\",\n",
    "    ]\n",
    "    _SPLIT_RE = re.compile(r\"(<\\|[^>]+?\\|>)\")\n",
    "\n",
    "    def __init__(self, tokenizer_file_path=\"tokenizer-base.json\",\n",
    "                 apply_chat_template=False,\n",
    "                 add_generation_prompt=False,\n",
    "                 add_thinking=False):\n",
    "        from tokenizers import Tokenizer\n",
    "\n",
    "        self.apply_chat_template = apply_chat_template\n",
    "        self.add_generation_prompt = add_generation_prompt\n",
    "        self.add_thinking = add_thinking\n",
    "\n",
    "        tok_path = Path(tokenizer_file_path)\n",
    "        if not tok_path.is_file():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Tokenizer file '{tok_path}' not found. Please ensure it's available.\"\n",
    "            )\n",
    "\n",
    "        self._tok = Tokenizer.from_file(str(tok_path))\n",
    "        self._special_to_id = {t: self._tok.token_to_id(t) for t in self._SPECIALS}\n",
    "\n",
    "        self.pad_token = \"<|endoftext|>\"\n",
    "        self.pad_token_id = self._special_to_id.get(self.pad_token)\n",
    "\n",
    "        # Match HF behavior: chat model → <|im_end|>, base model → <|endoftext|>\n",
    "        fname = tok_path.name.lower()\n",
    "        if \"base\" in fname and \"reasoning\" not in fname:\n",
    "            self.eos_token = \"<|endoftext|>\"\n",
    "        else:\n",
    "            self.eos_token = \"<|im_end|>\"\n",
    "        self.eos_token_id = self._special_to_id.get(self.eos_token)\n",
    "\n",
    "    def encode(self, prompt, chat_wrapped=None):\n",
    "        if chat_wrapped is None:\n",
    "            chat_wrapped = self.apply_chat_template\n",
    "\n",
    "        stripped = prompt.strip()\n",
    "        if stripped in self._special_to_id and \"\\n\" not in stripped:\n",
    "            return [self._special_to_id[stripped]]\n",
    "\n",
    "        if chat_wrapped:\n",
    "            prompt = self._wrap_chat(prompt)\n",
    "\n",
    "        ids = []\n",
    "        for part in filter(None, self._SPLIT_RE.split(prompt)):\n",
    "            if part in self._special_to_id:\n",
    "                ids.append(self._special_to_id[part])\n",
    "            else:\n",
    "                ids.extend(self._tok.encode(part).ids)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return self._tok.decode(token_ids, skip_special_tokens=False)\n",
    "\n",
    "    def _wrap_chat(self, user_msg):\n",
    "        s = f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
    "        if self.add_generation_prompt:\n",
    "            s += \"<|im_start|>assistant\"\n",
    "            if self.add_thinking:\n",
    "                s += \"\\n\"  # insert no <think> tag, just a new line\n",
    "            else:\n",
    "                s += \"\\n<think>\\n\\n</think>\\n\\n\"\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127874f7",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b93c9b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length, dtype=dtype)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0)  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "def apply_rope(x, cos, sin, offset=0):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2:]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[offset:offset + seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[offset:offset + seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    # It's ok to use lower-precision after applying cos and sin rotation\n",
    "    return x_rotated.to(dtype=x.dtype)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-6, bias=False, qwen3_compatible=True):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.qwen3_compatible = qwen3_compatible\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "\n",
    "        if self.qwen3_compatible:\n",
    "            x = x.to(torch.float32)\n",
    "\n",
    "        variance = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        norm_x = x * torch.rsqrt(variance + self.eps)\n",
    "        norm_x = norm_x * self.scale\n",
    "\n",
    "        if self.shift is not None:\n",
    "            norm_x = norm_x + self.shift\n",
    "\n",
    "        return norm_x.to(input_dtype)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            qk_norm=cfg[\"qk_norm\"],\n",
    "            dtype=cfg[\"dtype\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "\n",
    "    def forward(self, x, mask, cos, sin, start_pos=0, cache=None):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x, next_cache = self.att(x, mask, cos, sin, start_pos=start_pos, cache=cache)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x, next_cache\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = nn.functional.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False, dtype=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        if head_dim is None:\n",
    "            assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
    "            head_dim = d_in // num_heads\n",
    "\n",
    "        self.head_dim = head_dim\n",
    "        self.d_out = num_heads * head_dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
    "\n",
    "        if qk_norm:\n",
    "            self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "            self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "        else:\n",
    "            self.q_norm = self.k_norm = None\n",
    "\n",
    "    def forward(self, x, mask, cos, sin, start_pos=0, cache=None):\n",
    "        b, num_tokens, _ = x.shape\n",
    "\n",
    "        # Apply projections\n",
    "        queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)\n",
    "        keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        # Reshape to heads / kv-groups\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys_new = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "        values_new = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Optional normalization\n",
    "        if self.q_norm:\n",
    "            queries = self.q_norm(queries)\n",
    "        if self.k_norm:\n",
    "            keys_new = self.k_norm(keys_new)\n",
    "\n",
    "        # Apply RoPE\n",
    "        queries = apply_rope(queries, cos, sin, offset=start_pos)\n",
    "        keys_new = apply_rope(keys_new, cos, sin, offset=start_pos)\n",
    "\n",
    "        if cache is not None:\n",
    "            prev_k, prev_v = cache\n",
    "            keys = torch.cat([prev_k, keys_new], dim=2)\n",
    "            values = torch.cat([prev_v, values_new], dim=2)\n",
    "        else:\n",
    "            start_pos = 0  # reset RoPE\n",
    "            keys, values = keys_new, values_new\n",
    "        next_cache = (keys, values)\n",
    "\n",
    "        # Expand K and V to match number of heads\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        # Attention\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
    "\n",
    "        context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
    "        return self.out_proj(context), next_cache\n",
    "\n",
    "class Qwen3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # Main model parameters\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.trf_blocks = nn.ModuleList(  # ModuleList since Sequential can only accept one input, and we need `x, mask, cos, sin`\n",
    "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "        # Reusable utilities\n",
    "        if cfg[\"head_dim\"] is None:\n",
    "            head_dim = cfg[\"emb_dim\"] // cfg[\"n_heads\"]\n",
    "        else:\n",
    "            head_dim = cfg[\"head_dim\"]\n",
    "        cos, sin = compute_rope_params(\n",
    "            head_dim=head_dim,\n",
    "            theta_base=cfg[\"rope_base\"],\n",
    "            context_length=cfg[\"context_length\"]\n",
    "        )\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "        self.cfg = cfg\n",
    "        self.current_pos = 0  # Track current position in KV cache\n",
    "\n",
    "    def forward(self, in_idx, cache=None):\n",
    "        # Forward pass\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds\n",
    "\n",
    "        num_tokens = x.shape[1]\n",
    "        if cache is not None:\n",
    "            pos_start = self.current_pos\n",
    "            pos_end = pos_start + num_tokens\n",
    "            self.current_pos = pos_end\n",
    "            mask = torch.triu(\n",
    "                torch.ones(pos_end, pos_end, device=x.device, dtype=torch.bool), diagonal=1\n",
    "            )[pos_start:pos_end, :pos_end]\n",
    "        else:\n",
    "            pos_start = 0  # Not strictly necessary but helps torch.compile\n",
    "            mask = torch.triu(\n",
    "                torch.ones(num_tokens, num_tokens, device=x.device, dtype=torch.bool), diagonal=1\n",
    "            )\n",
    "        # Prefill (no cache): mask starts as (num_tokens, num_tokens)\n",
    "        # Cached decoding: mask starts as (num_tokens, prev_k_number_tokens + num_tokens)\n",
    "        #\n",
    "        # We add two leading dimensions so the mask becomes\n",
    "        # (1, 1, num_tokens, num_tokens) during prefill and\n",
    "        # (1, 1, num_tokens, total_key_tokens) during cached decoding.\n",
    "        # These extra dimensions let PyTorch broadcast the same mask\n",
    "        # across all batches and attention heads when applying it to\n",
    "        # attn_scores of shape (batch, num_heads, num_tokens, total_key_tokens).\n",
    "        mask = mask[None, None, :, :]  # broadcast mask\n",
    "\n",
    "        for i, block in enumerate(self.trf_blocks):\n",
    "            blk_cache = cache.get(i) if cache else None\n",
    "            x, new_blk_cache = block(x, mask, self.cos, self.sin,\n",
    "                                     start_pos=pos_start,\n",
    "                                     cache=blk_cache)\n",
    "            if cache is not None:\n",
    "                cache.update(i, new_blk_cache)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
    "        return logits\n",
    "\n",
    "    def reset_kv_cache(self):\n",
    "        self.current_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a918c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_device(enable_tensor_cores=True):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using NVIDIA CUDA GPU\")\n",
    "\n",
    "        if enable_tensor_cores:\n",
    "            major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "            if (major, minor) >= (2, 9):\n",
    "                torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "                torch.backends.cudnn.conv.fp32_precision = \"tf32\"\n",
    "            else:\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True\n",
    "                torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple Silicon GPU (MPS)\")\n",
    "\n",
    "    elif torch.xpu.is_available():\n",
    "        device = torch.device(\"xpu\")\n",
    "        print(\"Using Intel GPU\")\n",
    "\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    return device\n",
    "\n",
    "def download_file(url, out_dir=\".\", backup_url=None):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    filename = Path(urlparse(url).path).name\n",
    "    dest = out_dir / filename\n",
    "\n",
    "    def try_download(u):\n",
    "        try:\n",
    "            with requests.get(u, stream=True, timeout=30) as r:\n",
    "                r.raise_for_status()\n",
    "                size_remote = int(r.headers.get(\"Content-Length\", 0))\n",
    "\n",
    "                # Skip download if already complete\n",
    "                if dest.exists() and size_remote and dest.stat().st_size == size_remote:\n",
    "                    print(f\"✓ {dest} already up-to-date\")\n",
    "                    return True\n",
    "\n",
    "                # Download in 1 MiB chunks with progress display\n",
    "                block = 1024 * 1024\n",
    "                downloaded = 0\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=block):\n",
    "                        if not chunk:\n",
    "                            continue\n",
    "                        f.write(chunk)\n",
    "                        downloaded += len(chunk)\n",
    "                        if size_remote:\n",
    "                            pct = downloaded * 100 // size_remote\n",
    "                            sys.stdout.write(\n",
    "                                f\"\\r{filename}: {pct:3d}% \"\n",
    "                                f\"({downloaded // (1024*1024)} MiB / \"\n",
    "                                f\"{size_remote // (1024*1024)} MiB)\"\n",
    "                            )\n",
    "                            sys.stdout.flush()\n",
    "                if size_remote:\n",
    "                    sys.stdout.write(\"\\n\")\n",
    "            return True\n",
    "        except requests.RequestException:\n",
    "            return False\n",
    "\n",
    "    # Try main URL first\n",
    "    if try_download(url):\n",
    "        return dest\n",
    "\n",
    "    # Try backup URL if provided\n",
    "    if backup_url:\n",
    "        print(f\"Primary URL ({url}) failed.\\nTrying backup URL ({backup_url})...\")\n",
    "        if try_download(backup_url):\n",
    "            return dest\n",
    "\n",
    "    raise RuntimeError(f\"Failed to download {filename} from both mirrors.\")\n",
    "\n",
    "\n",
    "def download_qwen3_small(kind=\"base\", tokenizer_only=False, out_dir=\".\"):\n",
    "    files = {\n",
    "        \"base\": {\"model\": \"qwen3-0.6B-base.pth\", \"tokenizer\": \"tokenizer-base.json\"},\n",
    "        \"reasoning\": {\"model\": \"qwen3-0.6B-reasoning.pth\", \"tokenizer\": \"tokenizer-reasoning.json\"},\n",
    "    }\n",
    "    if kind not in files:\n",
    "        raise ValueError(\"kind must be 'base' or 'reasoning'\")\n",
    "\n",
    "    repo = \"rasbt/qwen3-from-scratch\"\n",
    "    hf_fmt = \"https://huggingface.co/{repo}/resolve/main/{file}\"\n",
    "    backup_root = \"https://f001.backblazeb2.com/file/reasoning-from-scratch/qwen3-0.6B\"\n",
    "    targets = [\"tokenizer\"] if tokenizer_only else [\"model\", \"tokenizer\"]\n",
    "\n",
    "    for key in targets:\n",
    "        fname = files[kind][key]\n",
    "        primary = hf_fmt.format(repo=repo, file=fname)\n",
    "        backup = f\"{backup_root}/{fname}\"\n",
    "        download_file(primary, out_dir=out_dir, backup_url=backup)\n",
    "\n",
    "\n",
    "\n",
    "QWEN_CONFIG_06_B = {\n",
    "    \"vocab_size\": 151_936,     # Vocabulary size\n",
    "    \"context_length\": 40_960,  # Length originally used during training\n",
    "    \"emb_dim\": 1024,           # Embedding dimension\n",
    "    \"n_heads\": 16,             # Number of attention heads\n",
    "    \"n_layers\": 28,            # Number of layers\n",
    "    \"hidden_dim\": 3072,        # Size of intermediate dim in FeedForward\n",
    "    \"head_dim\": 128,           # Size of the heads in GQA\n",
    "    \"qk_norm\": True,           # Whether to normalize queries & keys in GQA\n",
    "    \"n_kv_groups\": 8,          # Key-Value groups for GQA\n",
    "    \"rope_base\": 1_000_000.0,  # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,   # Lower-precision dtype to reduce memory\n",
    "}\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(which_model, device, use_compile, local_dir=\"qwen3\"):\n",
    "    if which_model == \"base\":\n",
    "\n",
    "        download_qwen3_small(\n",
    "            kind=\"base\", tokenizer_only=False, out_dir=local_dir\n",
    "        )\n",
    "\n",
    "        tokenizer_path = Path(local_dir) / \"tokenizer-base.json\"\n",
    "        model_path = Path(local_dir) / \"qwen3-0.6B-base.pth\"\n",
    "        tokenizer = Qwen3Tokenizer(tokenizer_file_path=tokenizer_path)\n",
    "\n",
    "    elif which_model == \"reasoning\":\n",
    "\n",
    "        download_qwen3_small(\n",
    "            kind=\"reasoning\", tokenizer_only=False, out_dir=local_dir\n",
    "        )\n",
    "\n",
    "        tokenizer_path = Path(local_dir) / \"tokenizer-reasoning.json\"\n",
    "        model_path = Path(local_dir) / \"qwen3-0.6B-reasoning.pth\"\n",
    "        tokenizer = Qwen3Tokenizer(\n",
    "            tokenizer_file_path=tokenizer_path,\n",
    "            apply_chat_template=True,\n",
    "            add_generation_prompt=True,\n",
    "            add_thinking=True,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid choice: which_model={which_model}\")\n",
    "\n",
    "    model = Qwen3Model(QWEN_CONFIG_06_B)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    if use_compile:\n",
    "        torch._dynamo.config.allow_unspec_int_on_nn_module = True\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f1eb67",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54c9dfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 12000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "def load_math_train(local_path=\"math_train.json\", save_copy=True):\n",
    "    local_path = Path(local_path)\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"math_full_minus_math500/refs/heads/main/\"\n",
    "        \"math_full_minus_math500.json\"\n",
    "    )\n",
    "\n",
    "    if local_path.exists():\n",
    "        with local_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "\n",
    "        if save_copy:  # Saves a local copy\n",
    "            with local_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "\n",
    "    return data\n",
    "\n",
    "math_train = load_math_train()\n",
    "\n",
    "print(\"Dataset size:\", len(math_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c697aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA CUDA GPU\n",
      "qwen3-0.6B-base.pth:   0% (7 MiB / 1433 MiB)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m get_device()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# device = torch.device(\"cpu\")\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhich_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_compile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m device \u001b[38;5;241m=\u001b[39m get_device()\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[10], line 125\u001b[0m, in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(which_model, device, use_compile, local_dir)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model_and_tokenizer\u001b[39m(which_model, device, use_compile, local_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen3\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m which_model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m         \u001b[43mdownload_qwen3_small\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m         tokenizer_path \u001b[38;5;241m=\u001b[39m Path(local_dir) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer-base.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m         model_path \u001b[38;5;241m=\u001b[39m Path(local_dir) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen3-0.6B-base.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[10], line 103\u001b[0m, in \u001b[0;36mdownload_qwen3_small\u001b[0;34m(kind, tokenizer_only, out_dir)\u001b[0m\n\u001b[1;32m    101\u001b[0m primary \u001b[38;5;241m=\u001b[39m hf_fmt\u001b[38;5;241m.\u001b[39mformat(repo\u001b[38;5;241m=\u001b[39mrepo, file\u001b[38;5;241m=\u001b[39mfname)\n\u001b[1;32m    102\u001b[0m backup \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackup_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 103\u001b[0m \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackup_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackup\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 74\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(url, out_dir, backup_url)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Try main URL first\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtry_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dest\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Try backup URL if provided\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 54\u001b[0m, in \u001b[0;36mdownload_file.<locals>.try_download\u001b[0;34m(u)\u001b[0m\n\u001b[1;32m     52\u001b[0m downloaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dest, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mblock):\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk:\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/my_tools/books/llm_rl/.venv/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/my_tools/books/llm_rl/.venv/lib/python3.10/site-packages/urllib3/response.py:1257\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m   1253\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp)\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder\u001b[38;5;241m.\u001b[39mhas_unconsumed_tail)\n\u001b[1;32m   1256\u001b[0m     ):\n\u001b[0;32m-> 1257\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1259\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1260\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/my_tools/books/llm_rl/.venv/lib/python3.10/site-packages/urllib3/response.py:1112\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m   1110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m-> 1112\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m data\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder\u001b[38;5;241m.\u001b[39mhas_unconsumed_tail)\n\u001b[1;32m   1120\u001b[0m ):\n",
      "File \u001b[0;32m~/my_tools/books/llm_rl/.venv/lib/python3.10/site-packages/urllib3/response.py:1028\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m   1025\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m-> 1028\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m   1030\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/my_tools/books/llm_rl/.venv/lib/python3.10/site-packages/urllib3/response.py:1011\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    which_model=\"base\",\n",
    "    device=device,\n",
    "    use_compile=False\n",
    ")\n",
    "\n",
    "device = get_device()\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_rlvr_grpo(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    math_data=math_train,\n",
    "    device=device,\n",
    "    steps=50,\n",
    "    num_rollouts=4,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    lr=1e-5,\n",
    "    checkpoint_every=5,\n",
    "    checkpoint_dir=\".\",\n",
    "    csv_log_path=\"train_rlvr_grpo_metrics.csv\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
